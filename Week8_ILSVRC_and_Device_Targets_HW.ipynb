{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools as it\n",
    "\n",
    "import os.path as osp\n",
    "import glob\n",
    "import helpers_08\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook incorporates a ton of new concepts.  As such, it is generally more of a walk-through of example code.  There are a few exercises, but we would strongly encourage you to modify the sample code here and experiment with it.  Also, the notebook is written to work on a pure CPU-only system.  There are several points where there are comments in the code with some instructions you can follow to run the code when you have a GPU available.  Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  `tf.variable_scope()`\n",
    "\n",
    "Throughout the class, we've created our variables directly, using `tf.Variable()`. This is the simplest way to use Variables, as it doesn't involve any programming \"magic\". However, TensorFlow includes another way to create Variables so that it is easier to access previously created Variables. It also forces you to be more precise with how you use Variables, and allows you to assign \"presets\" for various parameters in your Variables, such as the initialization values.\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_graph = tf.Graph()\n",
    "with var_graph.as_default(), tf.variable_scope('my_var_scope'):\n",
    "    w_init = tf.truncated_normal_initializer()\n",
    "    b_init = tf.zeros_initializer()\n",
    "    w = tf.get_variable('w', shape=[10, 10], initializer=w_init)\n",
    "    b = tf.get_variable('b', shape=[10], initializer=b_init)\n",
    "\n",
    "print(w,b,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates two variables, `w` and `b`, using the [`tf.get_variable()`](https://www.tensorflow.org/api_docs/python/tf/get_variable) method. The primary parameter is the string `name` of the `Variable` you'd like to retrieve. If a `Variable` in the scope already has that name, it will retrieve that `Variable` object. Otherwise, it will create that `Variable`. Because neither `w` nor `b` were created before, it creates them from scratch.\n",
    "\n",
    "##### Reusing variables\n",
    "\n",
    "Now if we want to reuse them at a later time, we can access them by calling the `variable_scope` again, and setting its `reuse` parameter to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with var_graph.as_default(),  tf.variable_scope('my_var_scope', reuse=True):\n",
    "    w_again = tf.get_variable('w')\n",
    "    b_again = tf.get_variable('b')\n",
    "\n",
    "print(w_again, b_again, sep=\"\\n\")\n",
    "print(w is w_again and b is b_again)  # \"is\" is object identity:  names refer to -same- object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we are within the same variable scope, we _must_ set `reuse` to `True`. If we don't, TensorFlow will complain at us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with var_graph.as_default(), tf.variable_scope('my_var_scope'):\n",
    "    try:\n",
    "        w_again = tf.get_variable('w')\n",
    "        b_again = tf.get_variable('b')\n",
    "    except ValueError as e:\n",
    "        print(str(e).splitlines()[0]) # clean it up for quick reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to passing `reuse` into the `variable_scope` parameter, we can set it after the fact by using the `variable_scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with var_graph.as_default(), tf.variable_scope('my_var_scope') as var_scope:\n",
    "    var_scope.reuse_variables()\n",
    "    w_again = tf.get_variable('w')\n",
    "    b_again = tf.get_variable('b')\n",
    "\n",
    "print(w_again, b_again, sep=\"\\n\")\n",
    "print(w is w_again and b is b_again)  # \"is\" is object identity:  names refer to -same- object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the current variable scope with `tf.get_variable_scope()`; similar to `tf.get_default_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with var_graph.as_default(), tf.variable_scope('my_var_scope'):\n",
    "    curr_scope = tf.get_variable_scope()\n",
    "    curr_scope.reuse_variables()\n",
    "    w_again = tf.get_variable('w')\n",
    "    b_again = tf.get_variable('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would you have to do this?  Imagine that you are a helper function that creates a layer.  If someone outside of you called you to make a layer and it had already defined the `variable_scope`, you might need to get access to that information.  `get_variable_scope` is how you'd do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variable initializers\n",
    "\n",
    "Here's the first example we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_graph = tf.Graph()\n",
    "with var_graph.as_default(), tf.variable_scope('my_var_scope') as scope:\n",
    "    w_init = tf.truncated_normal_initializer()\n",
    "    b_init = tf.zeros_initializer()\n",
    "    w = tf.get_variable('w', shape=[10, 10], initializer=w_init)\n",
    "    b = tf.get_variable('b', shape=[10], initializer=b_init)\n",
    "\n",
    "print(w,b,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used two Operations we've never seen before: `truncated_normal_initializer` and `zeros_initializer`. They are similar to what we've used in the past to initialize Variables: `truncated_normal` and `zeros`. The `initializer` Operations are designed to be used with `tf.get_variable()`.  They define a way to create an arbitrary initial value inside a Tensor, regardless of shape.\n",
    "\n",
    "Notice how we don't specify the shape of the `Variable` until we call `tf.get_variable()`. This separation allows us to reuse the same initialization `Operation` for multiple Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_graph = tf.Graph()\n",
    "with var_graph.as_default(), tf.variable_scope('my_var_scope') as scope:\n",
    "    # common initializer\n",
    "    w_init = tf.truncated_normal_initializer()\n",
    "\n",
    "    w1 = tf.get_variable('w1', shape=[10, 10], initializer=w_init)\n",
    "    w2 = tf.get_variable('w2', shape=[200], initializer=w_init)\n",
    "    w3 = tf.get_variable('w3', shape=[300,10,10], initializer=w_init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting default parameters\n",
    "\n",
    "Above, we use the line `initializer=w_init` over and over again. It would be nice if we could have that automatically be done for us. Luckily, we can! The `variable_scope()` function includes several options that we can provide as a default for any `Variables` we create inside that scope. To set `w_init` as our default initializer, we simple pass in `initializer=w_init` inside of the call to `variable_scope`. Then, we can leave the `initializer=` portion out of `get_variable()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_graph = tf.Graph()\n",
    "with var_graph.as_default():\n",
    "    w_init = tf.truncated_normal_initializer()\n",
    "    with tf.variable_scope('my_var_scope', initializer=w_init) as var_scope:\n",
    "        w1 = tf.get_variable('w1', shape=[10, 10])\n",
    "        w2 = tf.get_variable('w2', shape=[200])\n",
    "        w3 = tf.get_variable('w3', shape=[300,10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These default parameters can be nested, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_graph = tf.Graph()\n",
    "with var_graph.as_default():\n",
    "    init1 = tf.truncated_normal_initializer()\n",
    "    init2 = tf.zeros_initializer()\n",
    "    with tf.variable_scope('var_scope_1', initializer=init1) as var_scope_1:\n",
    "        w1 = tf.get_variable('w1', shape=[10, 10])\n",
    "        with tf.variable_scope('var_scope_2', initializer=init2) as var_scope_2:\n",
    "            w2 = tf.get_variable('w2', shape=[200])\n",
    "            w3 = tf.get_variable('w3', shape=[300,10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a `Session`, you can pass in a set of options in the form of a ConfigProto protocol buffer. You simply add the options you want to the `ConfigProto` initialization function. Here are a couple of the things you can do with the config proto:\n",
    "\n",
    "* Quietly place Ops on different devices if you explicitly call `with tf.device()` using a device that doesn't exist\n",
    "* Print where devices are placed as they are created\n",
    "* Tell TensorFlow to automatically use all of GPU memory immediately, but rather allocate it as necessary\n",
    "* Set a timeout for Operations\n",
    "\n",
    "We're going to make use of `log_device_placement` to keep write out (in the terminal) the placement of ops on devices.  And, in `allow_soft_placement` let's TensorFlow *try* to use your `device()` directives, but it can fall back if they aren't possible (bad hardware or an op doesn't exist for a device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple config\n",
    "sample_config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                             log_device_placement=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-part config\n",
    "gpu_config = tf.GPUOptions(allow_growth=True)\n",
    "multi_config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                            log_device_placement=True,\n",
    "                            gpu_options=gpu_config)\n",
    "sess = tf.Session(config=multi_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Device TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default in TensorFlow, Operations are automatically placed on a CPU or GPU (if available). In general, an Operation will be automatically placed on a GPU unless there isn't a GPU implementation of that `Operation` (assuming you have TensorFlow installed for GPUs).\n",
    "\n",
    "There can be a number of GOTCHAs when dealing with GPU setup.  For a nice collection of configuration tweaks, see:\n",
    "  * https://stackoverflow.com/documentation/tensorflow/10621/tensorflow-gpu-setup#t=201707111502593830241\n",
    "\n",
    "In particular, you may need to fiddle with `CUDA_VISIBLE_DEVICES`:\n",
    "  * http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What devices do I have available (and what are their names)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_local_devices is undocumented and may change/disappear\n",
    "def get_available_device_names():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_devices = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_devices]\n",
    "get_available_device_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Place on Device and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph = tf.Graph()\n",
    "with simple_graph.as_default():\n",
    "    with tf.device('/cpu:0'):\n",
    "        a = tf.constant(42)\n",
    "        b = tf.constant(10)\n",
    "\n",
    "    with tf.device('/cpu:0'): # adjust to /gpu:0 when you have a GPU capable system\n",
    "        c = tf.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, the logging will display in the terminal window where you started up jupyter\n",
    "simple_config=tf.ConfigProto(log_device_placement=True)   \n",
    "\n",
    "with tf.Session(graph=simple_graph, config=simple_config) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a demonstration of logging the Op placement on devices.  Unfortunately, we can't capture the output directly in a Jupyter notebook (not even with `%%capture` for you Jupyter super users).  So, you'll need to go to the terminal window where you started jupyter to see the logging messages.  But, they'll look like (we've cleaned up and added some whitespace to make it more readable):\n",
    "\n",
    "    2017-07-11 10:08:16.511206: I tensorflow/core/common_runtime/direct_session.cc:257] \n",
    "    Device mapping:\n",
    "    Mul: (Mul): /job:localhost/replica:0/task:0/cpu:0\n",
    "        2017-07-11 10:08:16.513781: I tensorflow/core/common_runtime/simple_placer.cc:841] \n",
    "        Mul:(Mul)/job:localhost/replica:0/task:0/cpu:0\n",
    "    Const_1: (Const): /job:localhost/replica:0/task:0/cpu:0\n",
    "        2017-07-11 10:08:16.513809: I tensorflow/core/common_runtime/simple_placer.cc:841] \n",
    "        Const_1:(Const)/job:localhost/replica:0/task:0/cpu:0\n",
    "    Const: (Const): /job:localhost/replica:0/task:0/cpu:0\n",
    "        2017-07-11 10:08:16.513817: I tensorflow/core/common_runtime/simple_placer.cc:841] \n",
    "        Const:(Const)/job:localhost/replica:0/task:0/cpu:0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Small-ish MultiDevice Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original Linear Regression\n",
    "To get us started, let's look at our old friend, linear regression solve by gradient descent.  To remind us what was going on there, here's some slightly streamlined code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_GD_LinearRegression:\n",
    "    def __init__(s):  \n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope('inputs'):\n",
    "                s.x_placeholder = tf.placeholder(tf.float32, [None, 2], name='x')\n",
    "                s.y_placeholder = tf.placeholder(tf.float32, [None], name='y')\n",
    "                s.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "\n",
    "            with tf.name_scope('model'):\n",
    "                s.w = tf.Variable(tf.truncated_normal([2, 1]), name='w')\n",
    "                s.b = tf.Variable(0.0, name='b')\n",
    "                s.y_hat = tf.matmul(s.x_placeholder, s.w) + s.b            \n",
    "                s.loss = tf.reduce_mean(tf.square(s.y_hat - s.y_placeholder), name='MSE')\n",
    "                s.train = tf.train.GradientDescentOptimizer(s.learning_rate).minimize(s.loss)\n",
    "            \n",
    "            s.init = tf.global_variables_initializer()\n",
    "        s.session = tf.Session(graph=graph)\n",
    "        s.session.run(s.init)\n",
    "    \n",
    "    def fit(s, train_dict):\n",
    "        return s.session.run([s.loss, s.w, s.b, s.train], feed_dict=train_dict)\n",
    "\n",
    "    def predict(s, test_dict):\n",
    "        return s.session.run(s.y_hat, feed_dict=test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-Device Linear Regression\n",
    "\n",
    "So, for using multiple devices in TensorFlow, we often make use of *data parallel* approch where we will duplicate the same model over several devices, do computations on those devices, merge the results, and then iterate.  In the following example, we'll duplicate the linear regression gradient calculations on individual devices and then (1) merge those results (average them) and (2) apply the merged gradients on a central device (typically the CPU in a single host environment).\n",
    "\n",
    "Here, we simulate the multiple devices as all being on the CPU.  However, you can adjust this to use (different) GPUs if you have that hardware.  In the TF docs, replicas of a model are often called \"towers\".  Our strategy is to compute the gradients on the towers (i.e., we go from training data to gradients).  Our next example is only going to run as far as computing the tower gradients and we're going to just look at the gradients coming off the first tower.  Trust us - it's complicated enough for one step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_LR_V1:\n",
    "    def __init__(s):  \n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope('inputs'):\n",
    "                s.x_placeholder = tf.placeholder(tf.float32, [None, 2], name='x')\n",
    "                s.y_placeholder = tf.placeholder(tf.float32, [None], name='y')\n",
    "                s.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "\n",
    "            # this effectively \"declares\" the parameters in a variable scope and explicitly\n",
    "            # places them on the cpu\n",
    "            with tf.variable_scope('parameters'), tf.device('/cpu:0'):\n",
    "                s.w = tf.get_variable('w', [2,1], initializer=tf.truncated_normal_initializer())\n",
    "                s.b = tf.get_variable('b', [1],   initializer=tf.zeros_initializer())\n",
    "            with tf.name_scope('train'):\n",
    "                s.opt = tf.train.GradientDescentOptimizer(s.learning_rate)\n",
    "                \n",
    "                # see _build_tower below; pass device= to change target device\n",
    "                # and/or change default device below\n",
    "                s.tower_grads = [s._build_tower('tower1'),\n",
    "                                 s._build_tower('tower2'),\n",
    "                                 s._build_tower('tower3')]\n",
    "                # we'll come back to these in a few minutes:\n",
    "                # s.avg_gradients = process_gradient_list(s.tower_grads)\n",
    "                # s.train = s.opt.apply_gradients(s.avg_gradients)\n",
    "            \n",
    "            \n",
    "            s.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # we could also pass a config into __init__\n",
    "        simple_config=tf.ConfigProto(log_device_placement=True)   \n",
    "        s.session = tf.Session(graph=graph)\n",
    "        s.session.run(s.init)\n",
    "\n",
    "    # NOTE:  change device parameter to device='/gpu:0' if on GPU capable machine\n",
    "    def _build_tower(s, name, device='/cpu:0'):\n",
    "        with tf.name_scope(name), tf.device(device):\n",
    "            # note, having a class somewhat reduces the need for get_variable/variable_scope\n",
    "            # but, we're doing this to show how the bigger examples (many of which are not\n",
    "            # object oriented work)\n",
    "            with tf.variable_scope('parameters', reuse=True):\n",
    "                w, b = tf.get_variable('w'), tf.get_variable('b')\n",
    "            y_hat       = tf.matmul(s.x_placeholder, w) + b\n",
    "            tower_loss  = tf.square(y_hat - s.y_placeholder)\n",
    "            tower_grads = s.opt.compute_gradients(tower_loss)\n",
    "            return tower_grads\n",
    "\n",
    "        \n",
    "    def get_one_gradient(s, train_dict):\n",
    "        return s.session.run(s.tower_grads[0], feed_dict=train_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 10\n",
    "x = np.random.normal(loc=1, scale=1, size=[NUM_EXAMPLES,2])\n",
    "epsilon = np.random.normal(loc=0, scale=1.5, size=[NUM_EXAMPLES])\n",
    "y = (3 * x[:,1]) + (2 * x[:,0]) + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradients From Tower 1 (only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multi_LR_V1()\n",
    "\n",
    "train_dict = {model.x_placeholder: x,\n",
    "              model.y_placeholder: y,\n",
    "              model.learning_rate: 0.001}\n",
    "\n",
    "grad_t1 = model.get_one_gradient(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_gradients returns a list of tuples like\n",
    "# [(gradient 1, variable 1), (gradient 2, variable 2), etc.]\n",
    "\n",
    "# for the first entry:\n",
    "grad_t1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prior cell output is hard to interpret, \n",
    "# until we realize that tensorflow has \"numpy-ified\" the variable\n",
    "# but we can see that the first variable there is the current value of \"w\"\n",
    "# which is the second element in the tuple in the Out[] immediately above\n",
    "model.w.eval(session=model.session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then for b \n",
    "# (note that our initializer for b was zero and \n",
    "#  the second tuple element here is 0)\n",
    "grad_t1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will need to take gradients coming off each tower (and for each variable) and \"merge\" them together into a single gradient to apply to our model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All the Gradients are belong to us\n",
    "\n",
    "Now, let's scale this up and deal with all of the gradients (and apply them).\n",
    "\n",
    "We are going to assume that the gradients from each tower should be equally weighted (as in the case when each tower seems the same number of examples).  If the towers saw different numbers of examples, we could compute a weighted average of the gradients.  As it is, we can simply average them.\n",
    "\n",
    "Unfortunately, the value of the gradients can be a bit odd - or even worse `None`.  Yes, instead of zero, sometimes the `compute_gradients` function will give us `(None, Variable)` pairs.  Ugh.  According to a comment at:  https://github.com/tensorflow/tensorflow/issues/783, `None` may mean:\n",
    "\n",
    "  * There is no connection from input to output.\n",
    "  * There is a connection, but it's through a discrete variable with meaningless gradients.\n",
    "  * There is a connection, but it's through an op that doesn't have an implemented gradient.\n",
    "  \n",
    "There is some debate in the bug thread about what cases should actually return `0.0`.  However, for our purposes, we just want something that works.  Here's how we'll handle it:  using the structure of the returned gradients (that we diagnosed above), we're going to pull out the individual values, replace `None`s with zero-filled tensors, and average them in.  Easy to say, but the code is a little annoying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gradient_list(gradient_list):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    gradient_list[0] looks like:  [(grad_1, var_1), (grad_2, var_2), ....]\n",
    "    listifying these looks like: [[(g1,v1), (g2,v2), ...],    list for tower 1\n",
    "                                  [(g1,v1), (g2,v2), ...],    list for tower 2\n",
    "                                  [(g1,v1), (g2,v2), ...]  ]  list for tower 3\n",
    "\n",
    "    Warning: gradient value (i.e., g1) can be None (ugh): \n",
    "    see https://github.com/tensorflow/tensorflow/issues/783\n",
    "    \n",
    "    zip(*gradient_list) will \"unzip\" this to:\n",
    "    [ [(g1, v1), (g1,v1), (g1,v1)]  list for variable/gradient 1\n",
    "      [(g2, v2), (g2,v2), (g2,v2)]] list for variable/gradient 2\n",
    "    output:\n",
    "    single list of: [(avg_grad_1, var_1), (avg_grad_2, var2), .... ]\n",
    "    \"\"\"\n",
    "    def n_to_z(gv_pair):\n",
    "        'convert None to appropriately shaped zero tensor'\n",
    "        grad, var = gv_pair\n",
    "        if grad is None:\n",
    "            grad = tf.zeros_like(var)\n",
    "        return grad\n",
    "    \n",
    "    avg_of_all_grads = []\n",
    "    for grads_for_one_var in zip(*gradient_list):\n",
    "        curr_variable = grads_for_one_var[0][1] # they all have same variable, grab one\n",
    "        curr_grads    = tf.stack([n_to_z(gv_pair) for gv_pair in grads_for_one_var]) # pull out grads\n",
    "        curr_avg      = tf.reduce_mean(curr_grads, axis=0)              # average them\n",
    "        avg_of_all_grads.append((curr_avg, curr_variable))\n",
    "    \n",
    "    return avg_of_all_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that helper, we can now get our replicated model version of linear regression running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_Multi_GD_LinearRegression:\n",
    "    def __init__(s):  \n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope('inputs'):\n",
    "                s.x_placeholder = tf.placeholder(tf.float32, [None, 2], name='x')\n",
    "                s.y_placeholder = tf.placeholder(tf.float32, [None], name='y')\n",
    "                s.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "\n",
    "            with tf.variable_scope('parameters'), tf.device('/cpu:0'):\n",
    "                s.w = tf.get_variable('w', [2,1], initializer=tf.truncated_normal_initializer())\n",
    "                s.b = tf.get_variable('b', [1],   initializer=tf.zeros_initializer())\n",
    "\n",
    "            with tf.name_scope('train'):\n",
    "                s.opt = tf.train.GradientDescentOptimizer(s.learning_rate)\n",
    "                \n",
    "                s.tower_grads = [s._build_tower('tower1'),\n",
    "                                 s._build_tower('tower2'),\n",
    "                                 s._build_tower('tower3')]\n",
    "                s.avg_gradients = process_gradient_list(s.tower_grads)\n",
    "                s.train = s.opt.apply_gradients(s.avg_gradients)\n",
    "            \n",
    "            \n",
    "            s.init = tf.global_variables_initializer()\n",
    "        s.session = tf.Session(graph=graph)\n",
    "        s.session.run(s.init)\n",
    "\n",
    "    # NOTE:  change device parameter to device='/gpu:0' if on GPU capable machine\n",
    "    def _build_tower(s, name, device='/cpu:0'):\n",
    "        with tf.name_scope(name), tf.device(device):\n",
    "            # note, having a class somewhat reduces the need for get_variable/variable_scope\n",
    "            # but, we're doing this to show how the bigger examples (many of which are not\n",
    "            # object oriented work)\n",
    "            with tf.variable_scope('parameters', reuse=True):\n",
    "                w, b = tf.get_variable('w'), tf.get_variable('b')\n",
    "            y_hat       = tf.matmul(s.x_placeholder, w) + b\n",
    "            tower_loss  = tf.square(y_hat - s.y_placeholder)\n",
    "            tower_grads = s.opt.compute_gradients(tower_loss)\n",
    "            return tower_grads\n",
    "\n",
    "        \n",
    "    def get_gradients(s, train_dict):\n",
    "        return s.session.run([s.tower_grads[0], s.tower_grads[1], s.tower_grads[2]], feed_dict=train_dict)\n",
    "        \n",
    "    def fit(s, test_dict):\n",
    "        return s.session.run([s.train], feed_dict=train_dict)\n",
    "    \n",
    "    def predict(s, test_dict):\n",
    "        return s.session.run(s.y_hat, feed_dict=test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TF_Multi_GD_LinearRegression()\n",
    "\n",
    "train_dict = {model.x_placeholder: x,\n",
    "              model.y_placeholder: y,\n",
    "              model.learning_rate: 0.001}\n",
    "\n",
    "results = model.get_gradients(train_dict)\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Records, Queues, and Coordinators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three topics are tightly related.  There are a few TensorFlow guides to give you some insight to their relationships:\n",
    "\n",
    "  * https://www.tensorflow.org/programmers_guide/reading_data\n",
    "  * https://www.tensorflow.org/programmers_guide/threading_and_queues\n",
    "\n",
    "Probably the major use case for Queues (and Records to feed Queues and Coordinator to drive Queues), is to keep data flowing efficiently to the computational devices (particularly GPUs).  This avoids (1) us writing custom code to cache and transit data from a file archive to our models, (2) let's the executing code all be written at the C++ level for speed.  Less code *and* more efficient?  Yes please!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per usual, we've got some helper code squirreled away that downloads the Tiny ImageNet dataset, which is a miniaturized and simplified version of the ILSVRC dataset. There are only 200 classes as opposed to 1000, and each of the files has been scaled to 64x64 pixels. We're mainly using it due to the fact that it is a smaller dataset!  We also have some code to extract out forms of the labels that we will need.\n",
    "\n",
    "We're going to move the data from the TIN data set format of \"directory of files\" to TFRecords.  There are two nice reasons to use TFRecords:  (1) they are a TensorFlow native format, so once you convert data to TFRecrods, you can mix-and-match that data to models without worrying about data conversion and (2) it's particularly easy to feed TFRecords to Queues.  Note, you can also feed other data, e.g., lines from csv files to Queues relatively easily.\n",
    "\n",
    "The general process is to gather multiple `Feature` instances into a `Features` and then into an `Example`.  The `Example` can be written out with a `TFRecordWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tin_path = helpers_08.grab_tin()\n",
    "id_to_synset, synset_to_id = helpers_08.get_tin_id_maps(tin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common helper function pattern for creating Features\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def make_example(img, lbl):\n",
    "    features = tf.train.Features(feature = {'image':_bytes_feature(img),\n",
    "                                            'label':_int64_feature(lbl)})\n",
    "    return tf.train.Example(features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tin_train_to_tfr(tin_path):\n",
    "    helpers_08.mkdir(osp.join(tin_path, \"tfr\"))\n",
    "    tfr_dest_path = osp.join(tin_path, \"tfr\", \"tin_train.tfr\")\n",
    "    base_src_path  = osp.join(tin_path, 'tiny-imagenet-200', \"train\")\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(tfr_dest_path)\n",
    "\n",
    "    for synset_dir in glob.glob(osp.join(base_src_path, \"n*\")):\n",
    "        int_label = synset_to_id[osp.basename(synset_dir)]\n",
    "        for filename in glob.glob(osp.join(synset_dir, \"images/*.JPEG\")):\n",
    "            with open(filename, 'rb') as f:\n",
    "                image_bytes = f.read()\n",
    "            example = make_example(image_bytes, int_label)\n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "# and \"go\"; comment out if you are rerunning the notebook\n",
    "# to save some time\n",
    "convert_tin_train_to_tfr(tin_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tin_validation_to_tfr(tin_path):\n",
    "    tfr_dest_path = osp.join(tin_path, \"tfr\", \"tin_valid.tfr\")\n",
    "    base_src_path  = osp.join(tin_path, 'tiny-imagenet-200', \"val\")\n",
    "    \n",
    "    label_filename = osp.join(base_src_path, \"val_annotations.txt\")\n",
    "    def proc_line(line):\n",
    "        filename, synset = line.split(None, 2)[:2]\n",
    "        label = synset_to_id[synset]\n",
    "        return filename, label\n",
    "    filename_to_label = dict(proc_line(a_line) for a_line in open(label_filename))\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(tfr_dest_path)\n",
    "    \n",
    "    for long_filename in glob.glob(osp.join(base_src_path, \"images/*.JPEG\")):\n",
    "        int_label = filename_to_label[osp.basename(long_filename)]\n",
    "        with open(long_filename, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "        example = make_example(image_bytes, int_label)\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "# and \"go\"; comment out if you are rerunning the notebook\n",
    "# to save some time\n",
    "convert_tin_validation_to_tfr(tin_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice with `TFRecords`, grab the MNIST character image data and convert it from the `*.npy` files to tensorflow Records.  Note, the data is probably in your `week_04/data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For converting MNIST to records:\n",
    "  * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py\n",
    "\n",
    "If you want to see how that gets used by a trainer:\n",
    "  * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said using Queues will reduce our coding overhead and help us efficiently feed our computing devices.  Here's a quick example of a pipeline that goes from TFRecords through some preprocessing and spits out training batches of images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can increase these for GPU systems; see docs for tf.train.shuffle_batch\n",
    "BATCH_SIZE = 5\n",
    "NUM_THREADS = 2\n",
    "CAPACITY = 100 + 3 * BATCH_SIZE\n",
    "\n",
    "def image_label_queue():\n",
    "    with tf.name_scope('input_queue'):\n",
    "        # create the list of filenames [see also: tf.train.match_filenames_once()]\n",
    "        # and queue them up\n",
    "        train_path = \"data/tin/tfr/tin_train.tfr\"\n",
    "        valid_path = \"data/tin/tfr/tin_valid.tfr\"\n",
    "        filenames = [train_path, valid_path]\n",
    "        filename_queue = tf.train.string_input_producer([train_path])\n",
    "\n",
    "        reader = tf.TFRecordReader()\n",
    "        key, example = reader.read(filename_queue)\n",
    "        # we gave these an image and a label above in make_example()\n",
    "        features = tf.parse_single_example(example, features={'image': tf.FixedLenFeature([], tf.string),\n",
    "                                                              'label': tf.FixedLenFeature([], tf.int64)})\n",
    "        image = features['image']\n",
    "        label = tf.to_int32(features['label'])\n",
    "\n",
    "        # PREPROCESS BEFORE ADDING TO QUEUE\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.image.resize_bilinear(image, [224, 224])\n",
    "        image = tf.squeeze(image)\n",
    "        image = tf.to_float(image)\n",
    "        image = (image - 127.5) / 127.5\n",
    "\n",
    "        # and queue it with shuffle_batch\n",
    "        image_batch, label_batch = tf.train.shuffle_batch([image, label], \n",
    "                                                          batch_size=BATCH_SIZE,\n",
    "                                                          num_threads=NUM_THREADS,\n",
    "                                                          capacity=CAPACITY,\n",
    "                                                          min_after_dequeue=15)\n",
    "        return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Coordinator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to show off how we *use* the Queues to feed our program.  There is a more complicated example coming with VGGNet below.  But here, we'll return to the humble linear regression code and feed it with a `tf.train.shuffle_batch` that is populated from a simple NumPy array (which gets automatically converted to a Tensor for us - as we saw in week 1).\n",
    "\n",
    "We're going to do one \"ugly\" thing in this example:  to feed the `shuffle_batch`, we're going to pass in the training data to init.  This isn't pretty, but the Coordinator introduces an issue.  To use the Coordinator, we need to `start_queue_runners` which in turn need their dependencies filled-in (aka, placeholders need to be fed).  But, feeding placeholders (and not doing anything else) is surprisingly trickly, so we're working around that.  There are hints that we could use a `placeholder` followed by a `Variable` to get values to `shuffle_batch`, but we were unable make that work (see \"Preloaded Data\" here: https://www.tensorflow.org/programmers_guide/reading_data\n",
    ").  Better luck to you!\n",
    "\n",
    "To make use of our Queue (which is returned by `tf.train.shuffle_batch`), we need to drive it.  Here is template code to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_fit(s, train_dict, steps):\n",
    "    coord = tf.train.Coordinator()\n",
    "    with coord.stop_on_exception():\n",
    "        threads = tf.train.start_queue_runners(sess=s.session, coord=coord)\n",
    "        for step in range(steps):\n",
    "            if coord.should_stop(): \n",
    "                break\n",
    "    s.session.run(s.train, feed_dict=train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That function becomes our `fit` method and away we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coord_LinearRegression:\n",
    "    def __init__(s, in_x, in_y):  \n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope('train_inputs'):\n",
    "                s.x_train, s.y_train = tf.train.shuffle_batch([in_x, in_y], \n",
    "                                                               batch_size=15,\n",
    "                                                               num_threads=2,\n",
    "                                                               capacity=100,\n",
    "                                                               min_after_dequeue=15,\n",
    "                                                               enqueue_many = True,\n",
    "                                                               allow_smaller_final_batch=True)\n",
    "\n",
    "                s.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "\n",
    "            with tf.name_scope('test_inputs'):\n",
    "                s.x_test = tf.placeholder(tf.float32, [None, 2], name='x_test')\n",
    "                s.y_test= tf.placeholder(tf.float32, [None], name='y_test')\n",
    "\n",
    "                \n",
    "            with tf.name_scope('train_model'):\n",
    "                s.w = tf.Variable(tf.truncated_normal([2, 1]), name='w')\n",
    "                s.b = tf.Variable(0.0, name='b')\n",
    "                s.y_hat_train = tf.matmul(s.x_train, s.w) + s.b\n",
    "                s.train_loss = tf.reduce_mean(tf.square(s.y_hat_train - s.y_train), name='MSE')\n",
    "                s.train = tf.train.GradientDescentOptimizer(s.learning_rate).minimize(s.train_loss)\n",
    "                \n",
    "            with tf.name_scope('test_model'):\n",
    "                s.y_hat_test = tf.matmul(s.x_test, s.w) + s.b\n",
    "\n",
    "                \n",
    "            \n",
    "            s.init = tf.global_variables_initializer()\n",
    "        s.session = tf.Session(graph=graph)\n",
    "        s.session.run(s.init)\n",
    "    \n",
    "    def fit(s, train_dict, steps):\n",
    "        coord = tf.train.Coordinator()\n",
    "        with coord.stop_on_exception():\n",
    "            threads = tf.train.start_queue_runners(sess=s.session, coord=coord)\n",
    "            for step in range(steps):\n",
    "                if coord.should_stop(): \n",
    "                    break\n",
    "                s.session.run(s.train, feed_dict=train_dict)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        # s.session.close() # keep it open so we can predict\n",
    "\n",
    "\n",
    "    def predict(s, test_dict):\n",
    "        return s.session.run(s.y_hat_test, feed_dict=test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EXAMPLES = 5000\n",
    "def make_y(x):\n",
    "    epsilon = np.random.normal(loc=0, scale=1.5, size=len(x)).astype(np.float32)\n",
    "    y = (3 * x[:,0]) + (2 * x[:,1]) + epsilon\n",
    "    return y\n",
    "\n",
    "# weirdness\n",
    "def make_input_data():\n",
    "    x_train = np.random.normal(loc=1, scale=1, size=[NUM_TRAIN_EXAMPLES,2]).astype(np.float32)\n",
    "    y_train = make_y(x_train)\n",
    "    return [x_train, y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Coord_LinearRegression(*make_input_data()) # detuple\n",
    "train_dict = {model.learning_rate: 0.001}\n",
    "model.fit(train_dict, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_EXAMPLES = 15\n",
    "x_test = np.random.normal(loc=1, scale=1, size=[NUM_TEST_EXAMPLES,2]).astype(np.float32)\n",
    "y_test = make_y(x_test)\n",
    "\n",
    "model.predict({model.x_test:x_test, model.y_test:y_test})[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting (and slightly aggravating) side-effect of using a Queue to provide our inputs (instead of `tf.placeholder`s), is that we've lost our ability to naturally feed test data to the model by setting the placeholders.  So, you may have noticed that in `Coord_LinearRegression`, we added a `name_scope` for `test_inputs`.  These *can* be set with `feed_dict` and they allow us to use our \"normal\" prediction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our end goal today is going to be a multi-device and Coordinated version of VGGNet.  That's a pretty big step from where we're at right now.  So, let's break that down into a (large) subtask.  Creating the VGGNet architecture.\n",
    "\n",
    "You can read about VGGNet in the [VGGNet paper on arXiv.org](https://arxiv.org/abs/1409.1556).  Here is the architecture of VGG.  The convolution layers use a *3x3* windows and max pool layers use a *2x2* window.\n",
    "\n",
    "|19 (Wgt) Layer VGG Network|\n",
    "|----------------|\n",
    "| 224 x 224 RGB  |\n",
    "| 2 conv-64      |\n",
    "| max pool |\n",
    "| 2 conv-128 |\n",
    "| maxpool|\n",
    "| 4 conv-256 | \n",
    "| max pool |\n",
    "| 4 conv-512 | \n",
    "| max pool | \n",
    "| 4 conv-512 |\n",
    "| max pool |\n",
    "| FC 4096 (w/dropout) |\n",
    "| FC 4096 (w/dropout) |\n",
    "| FC 1000 |\n",
    "| soft max|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading TensorBoard Graph for a Pre-built VGG Model\n",
    "\n",
    "Inside of the `prebuilt` folder, there is a TensorBoard graphs exported for VGGNet. You will use these as guidance for creating your own VGGNet. Fire up TensorBoard:\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=prebuilt\n",
    "```\n",
    "\n",
    "Navigate to `localhost:6006` in your browser. After you click on the \"Graphs\" link, you'll be able to switch to VGG from the drop-down \"Runs\" menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Older Layer functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `helpers_08.py`, we have gathered together versions of: \n",
    "\n",
    "  * `conv_layer`, \n",
    "  * `flatten`, \n",
    "  * `fully_connected_xavier_relu_layer`, and \n",
    "  * `pool_layer`.  \n",
    "  \n",
    "You can use these to build your implementation of VGGNet.  If you want a quick reminder of their code, in a Jupyter notebook can use `??helpers_08.pool_layer` to get the source code of `pool_layer` (and likewise for the other functions).  Of course, you can always open `helpers_08.py` in a text editor.\n",
    "\n",
    "We've made one set of modifications to `conv_layer` and `fully_connected_xavier_relu_layer`:  we've traded a `tf.name_scope` in each for `tf.variable_scope` so that the same layers in different replicas (towers) of our model will use the same parameters which are stored on the CPU.  We discussed this earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see helper_08.pool_layer code, uncomment the bottom line \n",
    "# (it will open a pop-up at the bottom of your browser):\n",
    "# ??helpers_08.pool_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above layer functions to recreate the VGGNet from the above architecture. Your model function should expect two parameter inputs:\n",
    "\n",
    "* `incoming`: a 4D tensor with dtype `float32` and shape `[batch_size, 224, 224, 3]`\n",
    "* `keep_prob`: A scalar `Tensor` with dtype `float32` representing the keep_probability for dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_net(incoming, keep_prob):\n",
    "    ### YOUR CODE HERE\n",
    "    pass\n",
    "    ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you think you have something reasonable, you can create the network and save it to disk.  Then, open it up in TensorBoard to check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A MultiGPU and Coordinated VGG Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now for all the marbles.  Here, we put together towers of models (which use `variable_scope` to share parameters) which we can place on devices as we want, we feed data to the model using Queues (fed from `TFRecord`s and controlled by a `Coordinator`).  But wait!  There's more!  We also hook in a VGGNet as the model.  Graduation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since it's integral to the model, this is the image_label_queue from above\n",
    "BATCH_SIZE = 5\n",
    "NUM_THREADS = 2\n",
    "CAPACITY = 100 + 3 * BATCH_SIZE\n",
    "\n",
    "def image_label_queue():\n",
    "    with tf.name_scope('input_queue'):\n",
    "        # create the list of filenames [see also: tf.train.match_filenames_once()]\n",
    "        # and queue them up\n",
    "        train_path = \"data/tin/tfr/tin_train.tfr\"\n",
    "        valid_path = \"data/tin/tfr/tin_valid.tfr\"\n",
    "        filenames = [train_path, valid_path]\n",
    "        filename_queue = tf.train.string_input_producer([train_path])\n",
    "\n",
    "        reader = tf.TFRecordReader()\n",
    "        key, example = reader.read(filename_queue)\n",
    "        features = tf.parse_single_example(example, features={'image': tf.FixedLenFeature([], tf.string),\n",
    "                                                              'label': tf.FixedLenFeature([], tf.int64)})\n",
    "        image = features['image']\n",
    "        label = tf.to_int32(features['label'])\n",
    "\n",
    "        # PREPROCESS BEFORE ADDING TO QUEUE\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.image.resize_bilinear(image, [224, 224])\n",
    "        image = tf.squeeze(image)\n",
    "        image = tf.to_float(image)\n",
    "        image = (image - 127.5) / 127.5\n",
    "\n",
    "        image_batch, label_batch = tf.train.shuffle_batch([image, label], \n",
    "                                                          batch_size=BATCH_SIZE,\n",
    "                                                          num_threads=NUM_THREADS,\n",
    "                                                          capacity=CAPACITY,\n",
    "                                                          min_after_dequeue=15)\n",
    "        return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_VGG_Model:\n",
    "    def __init__(s, num_towers, config):\n",
    "        DECAY_STEPS, DECAY_FACTOR = 100000, 0.998\n",
    "\n",
    "        multi_vgg_graph = tf.Graph()\n",
    "        with multi_vgg_graph.as_default(), tf.device('/cpu:0'):\n",
    "            with tf.name_scope('step'):\n",
    "                s.global_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=\"global_step\")\n",
    "                s.increment = s.global_step.assign_add(1)\n",
    "\n",
    "            with tf.name_scope('inputs'):\n",
    "                s.image_batch, s.label_batch = image_label_queue()\n",
    "                \n",
    "            with tf.name_scope('train'):\n",
    "                lr = tf.train.exponential_decay(0.05, s.global_step, \n",
    "                                                DECAY_STEPS, DECAY_FACTOR, \n",
    "                                                staircase=True)\n",
    "                s.opt = tf.train.AdamOptimizer(lr)\n",
    "                s.tower_grads = [s._build_tower('tower_{:02d}'.format(i), i==0) for i in range(num_towers)]\n",
    "                s.avg_gradients = process_gradient_list(s.tower_grads)\n",
    "                s.train = s.opt.apply_gradients(s.avg_gradients)\n",
    "            \n",
    "            with tf.name_scope('summaries'):    \n",
    "                for grad, var in s.avg_gradients:\n",
    "                    if grad is not None:\n",
    "                        tf.summary.histogram(var.op.name + '_gradients', grad)\n",
    "                for var in tf.trainable_variables():\n",
    "                    tf.summary.histogram(var.op.name, var)\n",
    "                s.merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "            s.saver = tf.train.Saver()                                 # for model checkpoints\n",
    "            s.init = tf.global_variables_initializer()\n",
    "\n",
    "        s.writer = tf.summary.FileWriter(\"vgg_status/multi_vgg\", graph=multi_vgg_graph) # for model summaries\n",
    "        s.session = tf.Session(graph=multi_vgg_graph, config=config)\n",
    "        s.session.run(s.init)\n",
    "    \n",
    "    # NOTE:  change device parameter to device='/gpu:0' if on GPU capable machine\n",
    "    def _build_tower(s, name, first_tower, device='/cpu:0'):\n",
    "        print(name, ' on ', device)\n",
    "        with tf.name_scope(name), tf.device(device):\n",
    "            labels_onehot=tf.one_hot(s.label_batch, 1000) # number of classes\n",
    "\n",
    "            #\n",
    "            # this is awkward, but basically, it is our mechanism to \n",
    "            # have the first tower -create- the shared variables in in the scopes\n",
    "            # and have subsequent towers -reuse- those variables\n",
    "            #\n",
    "            # it works by getting the current scope (very likely, the outermost \n",
    "            # -- unless we are called inside someone else's variable_scope)\n",
    "            # and setting it to reuse if and only if we are a later tower\n",
    "            #\n",
    "            with tf.variable_scope(tf.get_variable_scope()) as curr_scope:\n",
    "                if not first_tower:\n",
    "                    curr_scope.reuse_variables()\n",
    "                logits = vgg_net(s.image_batch)\n",
    "\n",
    "            smce = tf.nn.softmax_cross_entropy_with_logits\n",
    "            tower_loss = tf.reduce_mean(smce(labels=labels_onehot, logits=logits))\n",
    "            tower_grads = s.opt.compute_gradients(tower_loss)\n",
    "            return tower_grads\n",
    "\n",
    "    def fit(s, steps, summary_step = None, checkpoint_step = None):\n",
    "        # default these Nones to \"never\"\n",
    "        summary_step = summary_step if summary_step else steps + 1      \n",
    "        checkpoint_step = checkpoint_step if checkpoint_step else steps + 1\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=s.session, coord=coord)\n",
    "\n",
    "        with coord.stop_on_exception():\n",
    "            for step in range(steps):\n",
    "                # mod is zero and step > 0\n",
    "                do_summaries  = not step % summary_step and step\n",
    "                do_checkpoint = not step % checkpoint_step and step\n",
    "\n",
    "                if coord.should_stop(): \n",
    "                    break\n",
    "                if do_summaries:\n",
    "                    s.write_summaries()\n",
    "                else:\n",
    "                    s.session.run(s.train)\n",
    "                if do_checkpoint: \n",
    "                    s.save_checkpoint()\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        s.session.close()\n",
    "    \n",
    "    def write_summaries(s):\n",
    "        _, summaries, gs = s.session.run([s.train, s.merged_summaries, s.global_step])\n",
    "        s.writer.add_summary(summaries, global_step=gs)\n",
    "        s.writer.flush()\n",
    "    \n",
    "    def save_checkpoint(s):\n",
    "        s.saver.save(s.session, 'vgg_status/multi_vgg/multi_vgg.ckpt', \n",
    "                     s.global_step.eval(session=s.session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=tf.ConfigProto(allow_soft_placement=True)\n",
    "                      #log_device_placement=True) # substantial output b/c big model\n",
    "multi_vgg = Multi_VGG_Model(4, config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets 800% utilization on my 8 core machine\n",
    "# a few steps here is enough to \"crush\" my 8 core; but you can fly through\n",
    "# 10 steps on a Tesla GPU (AWS p2.xlarge) (COOLNESS!)\n",
    "multi_vgg.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show and Tell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As an example of how we can repurpose VGGNet, here is a skeleton of code to run Show and Tell.\n",
    "\n",
    "For a full working example in the TensorFlow code base, see here:\n",
    "  * https://github.com/tensorflow/models/tree/master/im2txt\n",
    "\n",
    "Note, that there is both `git:tensorflow/models` as its own project (with many interesting models in it) *and* the `git:tensorflow/tensorflow` project (the mian code base) and a (somewhat more boring) `tensorflow/models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VOCABULARY SETTINGS:  in/out and embedding sizes\n",
    "IN_VOCAB, IN_EMBED_SIZE = 10000, 100\n",
    "OUT_VOCAB, OUT_EMBED_SIZE = 10000, 4096\n",
    "\n",
    "## RNN SETTINGS\n",
    "RNN_WIDTH, RNN_DEPTH = 1000, 4\n",
    "MAX_LEN_INPUTS = MAX_LEN_LABELS = 10\n",
    "\n",
    "## TRAINING SETTINGS\n",
    "BATCH_SIZE = 32\n",
    "NUM_NEG_SAMPLES=50\n",
    "\n",
    "show_graph = tf.Graph()\n",
    "with show_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    labels = tf.placeholder(tf.int32, [None, MAX_LEN_LABELS])\n",
    "    labels_length = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    # Encoder\n",
    "    vgg_logits = vgg_net(inputs)\n",
    "    encoded_image = show_graph.get_tensor_by_name('fc_1/Relu:0')\n",
    "    \n",
    "    # Decoder\n",
    "    output_embedding = tf.Variable(tf.truncated_normal([OUT_VOCAB, OUT_EMBED_SIZE]))\n",
    "    w = tf.Variable(tf.truncated_normal([RNN_WIDTH, OUT_VOCAB]))\n",
    "    b = tf.Variable(tf.zeros([OUT_VOCAB]))\n",
    "    \n",
    "    cell_base = tf.contrib.rnn.GRUCell\n",
    "    dec_cells = [cell_base(RNN_WIDTH) for _ in range(RNN_DEPTH)]\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell(dec_cells)\n",
    "    \n",
    "    # Initial decoder state is zero, instead of encoder state (not an RNN encoder)\n",
    "    dec_state = tuple(tf.unstack(tf.zeros([RNN_DEPTH,tf.shape(labels)[0], RNN_WIDTH])))\n",
    "    # Initial input is the encoded image\n",
    "    output, dec_state = dec_cell(encoded_image, dec_state)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    # Don't append the output from the first LSTM\n",
    "    outputs, logits = [], []\n",
    "    prev = output\n",
    "    for i, label in enumerate(tf.unstack(labels, axis=1)):\n",
    "        logit = tf.matmul(prev, w) + b\n",
    "        logits.append(logit)\n",
    "        label_idx = tf.argmax(logit, 1)\n",
    "        label_emb = tf.nn.embedding_lookup(output_embedding, label_idx)\n",
    "        prev, dec_state = dec_cell(label_emb, dec_state)\n",
    "        if i > 0:\n",
    "            outputs.append(prev)\n",
    "\n",
    "    lengths_exp = tf.expand_dims(labels_length, 1)\n",
    "    mask = tf.reshape(tf.tile(tf.range(MAX_LEN_LABELS), [tf.shape(labels)[0]]), \n",
    "                      [-1, MAX_LEN_LABELS], \n",
    "                      name='mask_reshape')\n",
    "    mask = tf.to_float(tf.less(mask, lengths_exp))\n",
    "    \n",
    "    def s_loss(logits, labels):\n",
    "        logits = tf.reshape(logits, [-1, RNN_WIDTH], name='logit_s')\n",
    "        labels = tf.reshape(labels, [-1, 1], name='label_s')\n",
    "        return tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b,\n",
    "                                          labels=labels, inputs=logits,\n",
    "                                          num_sampled=NUM_NEG_SAMPLES, num_classes=OUT_VOCAB)\n",
    "    test = tf.convert_to_tensor(outputs)\n",
    "    outputs_tensor = tf.reshape(test, [-1, MAX_LEN_LABELS, RNN_WIDTH], name='logit_reshape')\n",
    "\n",
    "    labels_float = tf.to_float(labels)\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(outputs_tensor, labels_float, mask, softmax_loss_function=s_loss)\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, [])\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    inc_step = tf.assign_add(global_step, 1, name='inc_step')\n",
    "    \n",
    "    logits_tensor = tf.reshape(tf.convert_to_tensor(logits), [-1, MAX_LEN_LABELS, OUT_VOCAB])\n",
    "    softmax = tf.nn.softmax(logits_tensor)\n",
    "    predictions = tf.to_int32(tf.argmax(softmax, 2))\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
