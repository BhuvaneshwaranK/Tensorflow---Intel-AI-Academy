{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools as it\n",
    "import os.path as osp\n",
    "\n",
    "import helpers_06\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab and Load the Pre-existing Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next week, we are going to do full *transfer learning*.  But, for this week, we are going to take a different, simpler strategy.  In our model building up to this point, we've initialized the weights in our networks randomly (from a truncated normal distribution or something similar).  We're going to do much, much better today.  Instead of starting with random weights and training them, we're going to make use of weights that someone already did the hard work of training!  \n",
    "\n",
    "So, we're going to have a two part strategy:\n",
    "  1.  Create the architecture of AlexNet (that matches the weights someone else learned)\n",
    "  2.  Use those learned weights to initialize our network.\n",
    "  \n",
    "Then, boom, we have a working AlexNet without having to train it.  If you are unimpressed, realized that the inital training of the AlexNet in the acdemic paper took about six days on a machine with two highend GPUs.  We are really saving computational work here!\n",
    "\n",
    "Getting the weights is an exercise in frustration, so we've insulated you from that process with a helper.  Here goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights, readable_labels = helpers_06.fetch_alexnet_weights_and_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use your Python-fu to investigate `pretrained_weights` and `readable_labels`.  In particular:\n",
    "  1.  What type of thing is `pretrained_weights`?  \n",
    "  2.  What is its structure?\n",
    "  3.  If I tell you that the inner-most values come as `W,b` pairs, can you get the *shapes* of the coefficients?\n",
    "  4.  What's in `readable_labels`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights are stored as a dictionary of lists, with each entry storing the trained weight and bias values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values tell us the size of the network we need to create.  Fortunately, since we can use NumPy arrays (directly) to create Tensors, we don't have to copy these shapes down anywhere! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the weights ahead of time is very helpful -- if we can make use of them!  We've got to do something a little bit different.  Here's an old way that we recreated a convolutional layer.  Remember that we set the weights and biases to random values and a constant, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init_conv_layer(incoming, num_kernels, kernel_sz, \n",
    "                           strides=[1, 1], padding='SAME',\n",
    "                           bval=0.01, \n",
    "                           activation_fn=tf.nn.relu, \n",
    "                           name=None):\n",
    "    prev_outshape = incoming.shape.dims[-1].value\n",
    "    kshape = kernel_sz + [prev_outshape, num_kernels]\n",
    "\n",
    "    fan_in = np.prod(incoming.shape[1:]).value\n",
    "    xavier_stddev = np.sqrt(2.0 / fan_in)\n",
    "    \n",
    "    strides = [1] + strides + [1]\n",
    "    with tf.variable_scope(name, 'conv_layer'):\n",
    "        w = tf.Variable(tf.truncated_normal(kshape, stddev=xavier_stddev), name='kernel')\n",
    "        b = tf.Variable(tf.constant(bval, shape=[num_kernels]), name='bias')\n",
    "        conv = tf.nn.conv2d(incoming, w, strides, padding, name='conv')\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        return z if activation_fn is None else activation_fn(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a convolution layer that uses explictly given NumPy arrays to initialize its weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt stands for \"pre-trained\"\n",
    "def pt_conv_layer(incoming, init_weight, init_bias,\n",
    "                  strides=[1, 1], padding='SAME',\n",
    "                  activation_fn=tf.nn.relu, \n",
    "                  name=None):\n",
    "    'expects to be given numpy arrays or tensors for weight/bias'\n",
    "    strides = [1] + strides + [1]\n",
    "    with tf.variable_scope(name, 'conv_layer'):\n",
    "        #\n",
    "        # FILL ME IN\n",
    "        #\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Convolution\n",
    "To mimic the architecture of AlexNet, we can split the convolution layer.  When AlexNet was trained for real, this is the portion that got divided between two GPUs.  This isn't 100% necessary for us to use, because we are just filling in weights.  But it shows how you would do the splitting, if you ever have a need for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt stands for \"pre-trained\"\n",
    "def pt_split_conv_layer(incoming, init_weight, init_bias, \n",
    "                        strides=[1, 1], padding='SAME',\n",
    "                        activation_fn=tf.nn.relu, \n",
    "                        name=None):\n",
    "    strides = [1] + strides + [1]\n",
    "    with tf.variable_scope(name, 'split_conv_layer'):\n",
    "        w = tf.Variable(init_weight, name='kernel')\n",
    "        b = tf.Variable(init_bias, name='bias')\n",
    "        \n",
    "        if True: # ignoring the split part for now\n",
    "            i_a, i_b = tf.split(incoming, 2, 3, name='split_inputs')\n",
    "            w_a, w_b = tf.split(w, 2, 3, name='split_weights')\n",
    "\n",
    "            conv_a = tf.nn.conv2d(i_a, w_a, strides, padding, name='conv_a')\n",
    "            conv_b = tf.nn.conv2d(i_b, w_b, strides, padding, name='conv_b')\n",
    "            merge = tf.concat([conv_a, conv_b], 3)\n",
    "        else:\n",
    "            conv = tf.nn.conv2d(incoming, w, strides, padding, name='conv')        \n",
    "        \n",
    "        z = tf.nn.bias_add(merge, b)\n",
    "        a = activation_fn(z) if activation_fn is not None else z\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AlexNet Utility Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more layers that are useful for AlexNet.  Here, we make use of TensorFlow's built-in `relu_layer` to make a fully connected layer.  If you want to know where the magic constants in `alex_lrn_layer` come from, see:\n",
    "  * http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/myalexnet_forward_newtf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt stands for \"pre-trained\"\n",
    "def pt_alex_fc_layer(incoming, init_weights, init_biases, name=None):\n",
    "    'expects to be given numpy arrays or tensors for weight/bias'\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        return tf.nn.relu_layer(incoming, init_weights, init_biases, name=name)\n",
    "    \n",
    "def alex_lrn_layer(inputs):\n",
    "    'lrn with some magic constants'\n",
    "    return tf.nn.local_response_normalization(inputs, 2, 1.0, 2e-05, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as last week's, with a minor convenience tweak to prepend/append `1`s to the stride for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_layer(incoming, ksize, strides, padding='VALID',\n",
    "                pool_fn=tf.nn.max_pool, name=None):\n",
    "    # pooling layer with stride padding\n",
    "    ksize = [1] + ksize + [1]\n",
    "    strides = [1] + strides + [1]    \n",
    "    with tf.variable_scope(name, 'pool_layer'):\n",
    "        return pool_fn(incoming, ksize, strides, padding)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Defining the Main AlexNet Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the architecture of the main AlexNet component:\n",
    "\n",
    "![](images/alexnet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, here are the layer weight sizes (say that three times fast!) that we loaded above:\n",
    "\n",
    "    conv1: (11, 11, 3, 96)\n",
    "    conv2: (5, 5, 48, 256)\n",
    "    conv3: (3, 3, 256, 384)\n",
    "    conv4: (3, 3, 192, 384)\n",
    "    conv5: (3, 3, 192, 256)\n",
    "    fc6: (9216, 4096)\n",
    "    fc7: (4096, 4096)\n",
    "    fc8: (4096, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to put the architecture and the sizes together to make your own AlexNet.  If you can do it wihtout the following hints, you are a rockstart.\n",
    "  1.  The layer structure is (note, lrn and pool don't have weights):\n",
    "    *  conv, lrn, pool\n",
    "    *  split, lrn, pool\n",
    "    *  conv, split, split, pool, flat\n",
    "    *  full, full, and a special fully connected: `tf.nn.xw_plus_b`.\n",
    "  2.  The size and strides for the pool layers are `[3,3]` and `[2,2]` respectively.\n",
    "  3.  You can find the stride for the first conv in the diagram.\n",
    "  4.  If you are very stubborn and don't want to look at the solution, see here for more hints:\n",
    "    *   * http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/myalexnet_forward_newtf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(images, init_weights):\n",
    "    #\n",
    "    # FILL ME IN\n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Loaded Weights\" AlexNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got all the pieces we need to make use of an AlexNet model made from previously learned (and shared with us) weights.  We'll sneak one other piece into our `class`:\n",
    "\n",
    "    def save(s):\n",
    "        with s.graph.as_default():\n",
    "            tf.train.Saver().save(s.session, osp.join(_dir, \"alexnet\"))\n",
    "\n",
    "This will let us do: `mm.save('saved_models')` to save our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_Model:\n",
    "    def __init__(s, img_height, img_width, init_weights):  # non-standard, for abbreviation\n",
    "        s.graph = tf.Graph()\n",
    "        with s.graph.as_default():\n",
    "            with tf.name_scope('inputs'):\n",
    "                # consider two possibilities here:\n",
    "                # 1.  use a fixed image size [img_height x img_width]\n",
    "                # s.images = tf.placeholder(tf.float32, \n",
    "                #                           shape=[None, img_height, img_width, 3], name=\"images\")\n",
    "                # 2.  use variable image sizes -and- rescale the images to the same size\n",
    "                #     in a rescaling step (see name_scope('rescale')) ... note, one \"batch\"\n",
    "                #     of images must all be the same size of image\n",
    "                s.images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\"images\")\n",
    "\n",
    "                # note, we don't need labels, b/c the work is already done for us (no fitting/training)\n",
    "                \n",
    "            with tf.name_scope('rescale'):\n",
    "                s.scaled_images = tf.image.resize_images(s.images, [img_height, img_width])\n",
    "                \n",
    "\n",
    "            s.logits = alexnet(s.scaled_images, init_weights, debug=True)\n",
    "                        \n",
    "            with tf.name_scope('prediction'):\n",
    "                s.softmax    = tf.nn.softmax(s.logits, name=\"softmax\")\n",
    "                s.prediction = tf.cast(tf.arg_max(s.softmax, 1), tf.int32) # FIXME: unnecessary cast?\n",
    "\n",
    "            s.init = tf.global_variables_initializer()\n",
    "            \n",
    "        s.session = tf.Session(graph=s.graph)\n",
    "        s.session.run(s.init)\n",
    "\n",
    "    def save(s, _dir):\n",
    "        # fixme:  save(session) versus export_meta_graph\n",
    "        helpers_06.mkdir(_dir)\n",
    "        with s.graph.as_default():\n",
    "            tf.train.Saver().save(s.session, osp.join(_dir, \"alexnet\"))\n",
    "        \n",
    "        \n",
    "    def predict(s, test_dict):\n",
    "        pred, prob = s.session.run([s.prediction, s.softmax], feed_dict=test_dict)\n",
    "        return pred, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from above:\n",
    "# pretrained_weights, readable_labels = helpers_06.fetch_alexnet_weights_and_classes()\n",
    "mm = AlexNet_Model(227, 227, init_weights=pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grab Some Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = ['dog', 'laska', 'poodle']\n",
    "base_url = \"http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\"\n",
    "local_dir = \"data/alexnet/\"\n",
    "\n",
    "for img_name in img_names:\n",
    "    helpers_06.maybe_download(\"{}.png\".format(img_name), base_url, local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "import os.path as osp\n",
    "img = imread(osp.join(local_dir, 'dog.png'))[:,:,:3]\n",
    "plt.imshow(img)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our reconstituted model to label this nice dog.  You'll need two things to do it:\n",
    "  1.  We wanted indices into the labels (`readable_labels` from way above).  With the output `probs`, `top5_indices = np.argsort(probs, 1)[0][-5:]`.\n",
    "  2.  With only one image, we have to fake a batch.  Do this: `img_batch = np.expand_dims(img, 0)`.  Then, you can use `img_batch` as if it were multiple images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.save('saved_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we can bring that saved model back.  We'll play with a \"reloaded\" model more next week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder graph to \"rehydrate\" our freeze-dried AlexNet\n",
    "old_alex_graph = tf.Graph()\n",
    "with old_alex_graph.as_default():\n",
    "    # importing the graph will populate new_alex_graph\n",
    "    saver = tf.train.import_meta_graph(\"saved_models/alexnet.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice with Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some practice with optimizers, go back to one of your MNIST (or CIFAR) models.  The Optimizers are listed here:\n",
    "  * https://www.tensorflow.org/api_guides/python/train\n",
    "\n",
    "Experiment with using several different optimizers and see:\n",
    "  1.  learning performance\n",
    "  2.  convergance rate\n",
    "  3.  processing time (per step/epoch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
