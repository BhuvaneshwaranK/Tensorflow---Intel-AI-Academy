{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D # needed for project='3d'\n",
    "import helpers_02\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NumPy version of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get us started, let's investigate how gradient descent works.  We'll start by implementing it in NumPy. To keep things simple, we'll use a very basic cost function $J(x)=x^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function: x^2\n",
    "def J(x):\n",
    "    return x * x\n",
    "\n",
    "# Gradient of x^2 = 2*x\n",
    "def J_grad(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Creates datapoints to generate a gradient slope line\n",
    "def grad_graph_helper(fn, mid, error, delta):\n",
    "    x = np.linspace(mid-delta, mid+delta)\n",
    "    y = error - (mid - x) * fn(mid)\n",
    "    return x, y\n",
    "\n",
    "# Plots x^2 with a given gradient slope and annotations \n",
    "def make_plot(ax, x, error, grad):\n",
    "    grad_x, grad_y = grad_graph_helper(J_grad, x, error, 1)\n",
    "    xs = np.linspace(-10, 10, 300)\n",
    "    Js = J(xs)\n",
    "    \n",
    "    ax.set_xlabel('x'); ax.set_ylabel('J'); ax.grid(True)\n",
    "\n",
    "    ax.plot(xs, Js)\n",
    "    ax.plot(grad_x, grad_y, c='g', linewidth=2)\n",
    "    ax.scatter(x, error, c='r', marker='s', s=50, zorder=6)\n",
    "    \n",
    "    if grad < 0:\n",
    "        text_fill, x_add, error_add = (\"< 0\", \"right\"), 2, 10\n",
    "    else:\n",
    "        text_fill, x_add, error_add = (\"> 0\", \"left\"), -9, 10\n",
    "    ax.annotate(\"Grad is {}; move to the {}\".format(*text_fill),\n",
    "                [x, error], [x+x_add, error+error_add],\n",
    "                arrowprops={'arrowstyle': '->'})\n",
    "\n",
    "def descend(x, learning_rate, steps=50, graph_steps=[0, 5, 15, 49]):\n",
    "    x = float(x)\n",
    "    fig, axes = plt.subplots(ncols=1, \n",
    "                             nrows=len(graph_steps), \n",
    "                             figsize=(12, 4*len(graph_steps)))\n",
    "    axes = iter(axes)\n",
    "    for step in range(steps):\n",
    "        error, grad = J(x), J_grad(x)\n",
    "        if step in graph_steps:\n",
    "            ax = next(axes)\n",
    "            ax.set_title('Step: {}'.format(step))\n",
    "            make_plot(ax, x, error, grad)\n",
    "        x -= learning_rate * grad\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the left side\n",
    "descend(-9.0, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the right side\n",
    "descend(9.0, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if learning rate is too high\n",
    "descend(1.0, 1.3, 6, [2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create some toy data\n",
    "\n",
    "Let's create some data to learn from.  We're going to end up fitting a line to this data using linear regression.  The following code creates scatter data that is roughly along the line $y=3x+2$.  This is in the classing point-slope form of a line $y=mx+b$, but to prepare us for more general formulations, we'll write it as $y=wx+b$ ($w$ for *weight*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(loc=1, scale=1, size=[20])\n",
    "epsilon = np.random.normal(loc=0, scale=1.5, size=[20])\n",
    "y = (3 * x) + 2 + epsilon\n",
    "plt.scatter(x,y,c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a simple linear model that can learn to fit the previous data.  You know the drill by now.  We will:\n",
    "  1.  Create the model\n",
    "  2.  Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Graph for the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_graph = tf.Graph()\n",
    "with linreg_graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        x_placeholder = tf.placeholder(tf.float32, [None], name='x')\n",
    "        y_placeholder = tf.placeholder(tf.float32, [None], name='y')\n",
    "        learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "\n",
    "    with tf.name_scope('model'):\n",
    "        w = tf.Variable(tf.truncated_normal([]), name='w')\n",
    "        b = tf.Variable(tf.constant(0, dtype=tf.float32), name='b')\n",
    "        y_hat = tf.multiply(w, x_placeholder) + b\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.square(y_hat - y_placeholder), name='MSE')\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        opt = tf.train.GradientDescentOptimizer # long name!\n",
    "        train = opt(learning_rate).minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('global_step'):\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        inc_step = tf.assign_add(global_step, 1, name='inc_step')\n",
    "    \n",
    "    with tf.name_scope('helpers'):\n",
    "        init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by initializeing the Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session(graph=linreg_graph)\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we setup a simple training and testing scenario with some model data.  I've recreated the simulated data below for easy reference.  `train_dict` and `test_dict` will be fed into `run` calls.  They tie our python level data to Tensors in our graph (e.g., (e.g., `x_train` to  `x_placeholder`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"true\" model:  y = true_w * x + true_b\n",
    "true_w, true_b = 3,2\n",
    "\n",
    "# create training data and make a feeder dict\n",
    "x_train = np.random.uniform(-5, 5, size=[20])\n",
    "epsilon = np.random.normal(loc=0, scale=1.5, size=[20])\n",
    "y_train = (true_w * x_train) + true_b + epsilon\n",
    "train_dict = {x_placeholder: x_train,\n",
    "              y_placeholder: y_train,\n",
    "              learning_rate: 0.05}\n",
    "\n",
    "# some points to evaluate at \n",
    "# also serve to reconstruct fitted line below\n",
    "x_test = np.array([-5.0, 5.0])\n",
    "test_dict = {x_placeholder: x_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the \"main\" running of the model.  We have an loop that performs training every pass through.  Then, at selected steps (0, 1, 4, 10, 49), we also keep track of some results of testing on our evaluation `x_test` data.  In the `y_test=` line (line 10), we *do not* fetch the value of the `train` Operation.  Thus, we *do not* update the weights of our model.  Put another way, we only train when we fetch `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = []\n",
    "print(\"{:2} {:^6s}\".format(\"i\", \"MSE\"))\n",
    "for i in range(50):    \n",
    "    J, w_est, b_est, step, _ = session.run([loss, w, b, inc_step, train], \n",
    "                                           feed_dict=train_dict)\n",
    "    if not i % 5:\n",
    "        print(\"{:<2d} {:>6.3f}\".format(step, J))\n",
    "\n",
    "    if step in [0, 1, 4, 10, 49]:\n",
    "        y_test = session.run(y_hat, feed_dict=test_dict)\n",
    "        snapshots.append((i, y_test, w_est, b_est, J))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Progression of the Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final W and b values\n",
    "final_w_est, final_b_est = session.run([w, b])\n",
    "\n",
    "# extract out the sequences of w/b/J from the overall snapshots\n",
    "# PYTHON TRICKERY:  zip(*seq) does the \"opposite\" of zip()\n",
    "# here, it essentially pulls out the \"columns\" of snapshots\n",
    "_, _, w_ests, b_ests, losses = zip(*snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, snapshot in enumerate(snapshots):\n",
    "    # extract needed components for current snapshot\n",
    "    step, y_test, curr_w_est, curr_b_est, curr_loss = snapshot\n",
    "\n",
    "    # setup outer figure\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    title_fmt = 'Step: {} error: {:0.4f} w = {:0.4f} b = {:0.4f}'\n",
    "    title_string = title_fmt.format(step, curr_loss, \n",
    "                                    curr_w_est, curr_b_est)\n",
    "    fig.suptitle(title_string, size=20)\n",
    "\n",
    "    # Scatter plot of data with predicted line\n",
    "    ax = plt.subplot(131)\n",
    "    ax.set_xlabel('x'); ax.set_ylabel('y'); ax.grid(True)\n",
    "    ax.scatter(x_train, y_train, c='r')\n",
    "    ax.plot(x_test, y_test) # both curr train->test and estimated line\n",
    "\n",
    "    # 3D view of estimated w and b values against loss\n",
    "    ax = plt.subplot(132, projection='3d')\n",
    "    ax.set_xlabel('w'); ax.set_ylabel('b'); ax.set_zlabel('loss')\n",
    "    ax.plot(w_ests, b_ests, losses)\n",
    "    ax.scatter(curr_w_est, curr_b_est, curr_loss, c='r')\n",
    "    \n",
    "    # Overhead view of approximate error curves\n",
    "    ax = plt.subplot(133)\n",
    "    ax.set_xlabel('w'); ax.set_ylabel('b'); ax.grid(True)\n",
    "    for i in range(1,6):\n",
    "        circ = mpl.patches.Circle([final_w_est, final_b_est], \n",
    "                                  .5 * i, fill=False)\n",
    "        ax.add_patch(circ)\n",
    "        \n",
    "    ax.plot(w_ests, b_ests)                   # the path of w/b estimates\n",
    "    ax.scatter(curr_w_est, curr_b_est, c='r') # really just one point\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit learn's estimates\n",
    "import sklearn.linear_model as sk_lm\n",
    "model = sk_lm.LinearRegression()\n",
    "model.fit(np.reshape(x_train,[20,1]), y_train)\n",
    "print('sk linreg w: {:5.3f} b: {:5.3f}'.format(model.coef_[0], \n",
    "                                                model.intercept_))\n",
    "\n",
    "# compared with our estimates\n",
    "print('our tflr  w: {:5.3f} b: {:5.3f}'.format(final_w_est, \n",
    "                                                final_b_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code gets the gradient of the `tf.square()` Operation, which we expect to return `2*input`, as the derivative of $x^2$ is $2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_graph = tf.Graph()\n",
    "with grad_graph.as_default():\n",
    "    # A dummy variable that we can play around with to get different values\n",
    "    a = tf.Variable(3.0)\n",
    "\n",
    "    # The operation we want to get the derivative of\n",
    "    # b = a**2; deriv(b) = deriv(a**2) = 2a \n",
    "    # eval @ a=3 gives deriv(b wrt a) = 6\n",
    "    b = tf.square(a) \n",
    "\n",
    "    # Create an Optimizer\n",
    "    opt = tf.train.GradientDescentOptimizer(0.05)\n",
    "\n",
    "    # Compute the gradient of `b` with respect to `a`\n",
    "    grads = opt.compute_gradients(b, [a])\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    \n",
    "with tf.Session(graph=grad_graph) as session:\n",
    "    session.run(init)\n",
    "    print(session.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Realistic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the UCI Housing dataset for this example, where the target is to predict the median value of homes in neighborhoods of Boston. The below code downloads the dataset and information files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "base_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "data_suf = 'housing/housing.{}'\n",
    "url = base_url + data_suf\n",
    "\n",
    "data_path = helpers_02.download(url.format('data'), data_dir)\n",
    "helpers_02.download(url.format('names'), data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables in the dataset are defined in `data/housing.names`.  You can read through that file, but we've extracted the important parts here.\n",
    "\n",
    "1. **CRIM**: per capita crime rate by town\n",
    "2. **ZN**: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. **INDUS**: proportion of non-retail business acres per town\n",
    "4. **CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. **NOX**: nitric oxides concentration (parts per 10 million)\n",
    "6. **RM**: average number of rooms per dwelling\n",
    "7. **AGE**: proportion of owner-occupied units built prior to 1940\n",
    "8. **DIS**: weighted distances to five Boston employment centres\n",
    "9. **RAD**: index of accessibility to radial highways\n",
    "10. **TAX**: full-value property-tax rate per \\$10,000\n",
    "11. **PTRATIO**: pupil-teacher ratio by town\n",
    "12. **B**: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. **LSTAT**: \\% lower status of the population\n",
    "14. **MEDV**: Median value of owner-occupied homes in $1000's\n",
    "\n",
    "Now, we read in the data using Pandas powerful `pd.read_table` function and label the columns with manually extracted column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually extracted from housing.names\n",
    "housing_names = ('CRIM,ZN,INDUS,CHAS,NOX,'\n",
    "                 'RM,AGE,DIS,RAD,TAX,'\n",
    "                 'PTRATIO,B,LSTAT,MEDV').split(',')\n",
    "\n",
    "my_data = pd.read_table(data_path, sep='\\s+', names=housing_names)\n",
    "\n",
    "# one way to shuffle the data\n",
    "# data = data.reindex(np.random.permutation(data.index))\n",
    "my_data.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick way to make a single train/test split of the overall DataFrame.  This has some benefits over scikit_learn's `train_test_split` because our method does not shuffle the data (skl shuffles the indices and uses fancy indexing to select the train and test sets - this triggers a copy).  The result is that our method doesn't require copies which might be important with bigger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why not just use sklearn shuffle split?  \n",
    "# skl uses fancy indexing which triggers copies\n",
    "# these are continguous blocks, so there is no copy needed\n",
    "def df_train_test_blocks(df, pct):\n",
    "    num_train_examples = int(len(df) * pct)\n",
    "    return df.iloc[:num_train_examples], df.iloc[num_train_examples:]\n",
    "my_train, my_test = df_train_test_blocks(my_data, .8)\n",
    "my_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some more realistic data and we've created train/test subsets of the data, let's do some (linear regression) modeling!  Let's pick a few features (number of rooms, age, and tax rate) and (1) look at some descriptive statistics for those features and (2) look at the pairwise interactions between those features and median value with some scatter plots.  These graphs are only for the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ftrs = my_train[['MEDV', 'AGE', 'RM', 'TAX']]\n",
    "my_ftrs.describe()\n",
    "\n",
    "import seaborn as sns\n",
    "sns.pairplot(my_ftrs, x_vars=['AGE', 'RM', 'TAX'], y_vars=['MEDV']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Models with `scikit_learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models:  price ~ age, price ~ rooms, price ~ tax\n",
    "# scatter, regression line, r^2\n",
    "# more models:  saturated (all), price ~ rooms + tax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make three regression models between these variables (individually) and median price.  So, our three models will be (where $y \\sim x$ is read as \"y is modeled on x\", this notation comes from R): \n",
    "  1. $price \\sim  age$\n",
    "  1. $price \\sim rooms$\n",
    "  3. $price \\sim tax$\n",
    "  \n",
    "In turn, we'll use the training data to build each of these models (using scikit_learn) and then we'll graph the fit line against the training data.  We'll also compute the $R^2$ statistic for the fit against the training data ($R^2$ is SKL's default scorer for LinearRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_on = ['AGE', 'RM', 'TAX']\n",
    "models = {}\n",
    "fig, axes = plt.subplots(3,1)\n",
    "for ax, ftr in zip(axes, model_on):\n",
    "    models[ftr] = sk_lm.LinearRegression().fit(my_ftrs[[ftr]], \n",
    "                                               my_ftrs[['MEDV']])\n",
    "    ax.scatter(my_ftrs[[ftr]], my_ftrs[['MEDV']], c='r')\n",
    "\n",
    "    x_vals = np.array(ax.get_xlim()) # great trick to get min/max x\n",
    "    y_vals = models[ftr].intercept_ + models[ftr].coef_[0] * x_vals\n",
    "    ax.plot(x_vals, y_vals, c='b')\n",
    "\n",
    "    train_r_squared = models[ftr].score(my_ftrs[[ftr]], \n",
    "                                       my_ftrs[['MEDV']])\n",
    "    \n",
    "    ax.set_xlabel(ftr); ax.set_ylabel('MEDV')\n",
    "    ax.set_title(\"MEDV ~ {}\\nTrain $R^2$ = {:5.4f}\".format(ftr, train_r_squared))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, typically, we don't expect single variable to fully capture complex phenomena. So, let's build two more models:\n",
    "  \n",
    "  1. $price \\sim age + room + tax$\n",
    "  2. $price \\sim room + tax$\n",
    "  \n",
    "Here, adding something to the right hand side of the model means it is an additional feature/variable in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (('Full', ['AGE', 'RM', 'TAX']),\n",
    "          ('Room, Tax', ['RM', 'TAX']))\n",
    "target = 'MEDV'\n",
    "\n",
    "for model_name, predictors in models:\n",
    "    fit = sk_lm.LinearRegression().fit(my_ftrs[predictors],\n",
    "                                       my_ftrs[target])\n",
    "    r2 = fit.score(my_ftrs[predictors], my_ftrs[target])\n",
    "    print(\"{:10} Train R^2: {:5.4f}\".format(model_name, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data Modeling with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The room and tax model seems to do fairly well (at least compared to all three variables).  So, let's recreate that with TensorFlow.  We are going to add one layer of abstraction to our model.  You'll notice that the scikit_learn `Linear Regression` has three steps:\n",
    "\n",
    "  1.  create it\n",
    "  2.  fit it\n",
    "  3.  predict/score with it\n",
    "  \n",
    "These are very similar to the way we will typically use a TensorFlow model.  We've had two types of *running* our models that correspond to the fit and predict (train and test) steps for scikit learn.  The way we will implement this (and simplify the code that *uses* our model) is by create a simple `class` which has three methods:\n",
    "\n",
    "  1. `__init__`:  create the TensorFlow Graph\n",
    "  2. `fit`:  read in training data and optimize to it\n",
    "  3. `predict`:  read in testing data and give our best guesses (and possibly other results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_GD_LinearRegression:\n",
    "    # WARNING: \n",
    "    # s instead of self is non-standard, for abbreviation purposes\n",
    "    def __init__(s):  \n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope('inputs'):\n",
    "                s.x_placeholder = tf.placeholder(tf.float32, \n",
    "                                                 [None, 2], name='x')\n",
    "                s.y_placeholder = tf.placeholder(tf.float32, \n",
    "                                                 [None], name='y')\n",
    "                s.learning_rate = tf.placeholder(tf.float32, \n",
    "                                                 [], name='learning_rate')\n",
    "            with tf.name_scope('model'):\n",
    "                s.w = tf.Variable(tf.truncated_normal([2, 1]), name='w')\n",
    "                s.b = tf.Variable(0.0, name='b')\n",
    "                s.y_hat = tf.matmul(s.x_placeholder, s.w) + s.b\n",
    "            \n",
    "            with tf.name_scope('loss'):\n",
    "                s.loss = tf.reduce_mean(tf.square(s.y_hat - s.y_placeholder), name='MSE')\n",
    "            \n",
    "            with tf.name_scope('train'):\n",
    "                s.train = tf.train.GradientDescentOptimizer(s.learning_rate).minimize(s.loss)\n",
    "            \n",
    "            s.init = tf.global_variables_initializer()\n",
    "        s.session = tf.Session(graph=graph)\n",
    "        s.session.run(s.init)\n",
    "    \n",
    "    def fit(s, train_dict):\n",
    "        return s.session.run([s.loss, s.w, s.b, s.train], feed_dict=train_dict)\n",
    "\n",
    "    def predict(s, test_dict):\n",
    "        return s.session.run(s.y_hat, feed_dict=test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least three major advantages to structuring our code in a class:\n",
    "  1.  We have clearly delineated what the capabilities of the code are:  creation, fit, and prediction.\n",
    "  2.  Instead of \"polluting\" our top-level code (our main program) with many model Tensors, they are all contained within the class.  If we *must* access them in the main program, we can always access them with `object.Tensor` (there's an example coming below when we create the training and testing dictionaries).\n",
    "  3.  Although we don't do it here, if we needed to make many \"same but different\" models, we could use the same structure to create the models and passing in parameters to control the differences.\n",
    "  \n",
    "Enough talk.  Let's do this thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data (just a reminder from what was above)\n",
    "print(\"Data file:\", data_path)\n",
    "print(\"Names:\", housing_names)\n",
    "my_data = pd.read_table(data_path, sep='\\s+', names=housing_names)\n",
    "\n",
    "# select out some columns of interest and then train/test split it\n",
    "my_ftrs = my_data[['MEDV', 'AGE', 'RM', 'TAX']]\n",
    "my_train, my_test = df_train_test_blocks(my_ftrs, .8)\n",
    "\n",
    "# our ftrs to use for this particular model\n",
    "predictors = ['RM', 'TAX']\n",
    "target     = 'MEDV' # output tensor (y_placeholder) expects 1D tensor, not 2D - so no list\n",
    "\n",
    "# feeding data to TensorFlow, might not be happy with DataFrames\n",
    "# so we access the underlying NumPy data directly with .values\n",
    "my_train_x = my_train[predictors].values\n",
    "my_train_y = my_train[target].values\n",
    "my_test_x  = my_test[predictors].values\n",
    "my_test_y  = my_test[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TF_GD_LinearRegression()\n",
    "\n",
    "# model.x_placeholder becomes s.x_placeholder in TF_GD_LinearRegression above\n",
    "train_dict = {model.x_placeholder: my_train_x,\n",
    "              model.y_placeholder: my_train_y,\n",
    "              model.learning_rate: 0.000001}\n",
    "\n",
    "test_dict = {model.x_placeholder: my_test_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:3} {:>15s}\".format(\"i\", \"Training MSE\"))\n",
    "snapshots = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss, w, b, _ = model.fit(train_dict)\n",
    "    if epoch % 100 == 0 or epoch < 10:\n",
    "        print(\"{:<3d} {:>15.3f}\".format(epoch, loss))\n",
    "        y_test = model.predict(test_dict)\n",
    "        snapshots.append((epoch, y_test, w, b, loss))\n",
    "print(\"{:<2d} {:>15.3f}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the final trained model and use it to make predictions on our test set.  We'll then evaluate the quality of those predictions.  For ease of comparison, we'll compute the MSE (mean squared error) and use that as our metric.  You might like to play around with adding more iterations (epochs) to the training phase above and see how many iterations it takes to match scikit_learn's performance.  As an estimate, 100,000 epochs was getting closer to scikit learn's level, but didn't match it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final trained model\n",
    "import sklearn.metrics as sk_metrics\n",
    "y_test = model.predict(test_dict)\n",
    "tf_roomtax_mse = sk_metrics.mean_squared_error(y_test[:,0], my_test_y)\n",
    "\n",
    "print(\"tensorflow gd linear regression model\")\n",
    "print(\"price ~ room + tax\")\n",
    "print(\"w: {} b: {}\".format(w[:,0],b))\n",
    "print(\"Test MSE:\", tf_roomtax_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for comparison, with the scikit learn room+tax model\n",
    "skl_fit = sk_lm.LinearRegression().fit(my_train_x, my_train_y)\n",
    "skl_roomtax_mse = sk_metrics.mean_squared_error(skl_fit.predict(my_test_x), my_test_y)\n",
    "\n",
    "print(\"sklearn linear regression model:\")\n",
    "print(\"price ~ room + tax\")\n",
    "print(\"w: {} b: {}\".format(skl_fit.coef_, skl_fit.intercept_))\n",
    "print(\"Test MSE: {}\".format(skl_roomtax_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1: Access the Wine Quality Data\n",
    "\n",
    "Using `helpers_02.download` and the URL shown in a second, grab the [UCI Wine Quality data set](https://archive.ics.uci.edu/ml/datasets/Wine+Quality).  We are only going to focus on the white wines, but you are free to grab both files.  The wine quality data files live at:\n",
    "  \n",
    "  * https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
    "  * https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
    "\n",
    "There is also a `winequality.names` file in that directory.  You should be able to use the `base_url` that we defined earlier to assist you in creating a url for the `download` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 2:  Load the Data with Pandas\n",
    "\n",
    "Read the data into a DataFrame with Pandas.  `pd.read_csv` would be very useful here.  Note, it has an option to specify the delimiter (and the wine csv files are not comma separated).  One other note, you should specify a `dtype` to `read_csv`:  the quality value in the dataset is given as an integer, but our model (a regression model) is expecting a float value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 3:  Create a Train-Validation-Split Function\n",
    "\n",
    "In some instances (for example, when we need to set \"hyperparameters\" which are not optimized by our Graph), we may want a separate validation set.  Using `df_train_test_blocks` as a model, create `df_train_valid_test_blocks` which splits the data into three contiguous subsets.  As a reminder, if you wanted this splitting to be randomized, you would shuffle everything first and *then* split it.\n",
    "\n",
    "Create `x_train`, `x_test`, `y_train`, and `y_test`.  Feel free to choose whatever predictors you would like.  However, make it interesting, choose at least four or more.  You might like to go to all.  Doing that conveniently requires a bit of Pandas-fu.  If you struggle, feel free to look at the solutions - this isn't a Pandas test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4:  Create a Multivariate Linear Model in TensorFlow\n",
    "\n",
    "Create a Linear Regression model that is appropriate for the training data you selected out. Using `TF_GD_LinearRegression` will get you started, and in fact, it will get you almost all of the way there.  The model we constructed above was designed for two (and only two) predictors.  You will need to change it to accept more predictors.  [Hint:  there are *two* values in the Graph creation process you need to adjust.]\n",
    "\n",
    "If you are feeling up to it, you can make that modification a parameter to `__init__` so you can easily create models for different numbers of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 5:  Train and Test your Model\n",
    "\n",
    "'nuff said.  Try different learning rates and numbers of epochs.  Consider using different subsets of predictors.  See how well you can do (in terms of minimizing your test MSE).  You might want to compare your results with scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 6: Details, Details\n",
    "Finally, add some TensorBoard features to your Linear Regression model.\n",
    "\n",
    "* Add a scalar summary for your loss\n",
    "* Open up a FileWriter to save summaries to disk\n",
    "* Periodically write summary data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any hyperparameters, so I'm going to save data and not make a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
