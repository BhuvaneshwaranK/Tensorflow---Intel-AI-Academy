{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "\n",
    "import helpers_03\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurons as Logic Gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an introduction to neural networks and their component neurons, we are going to look at using neurons to implement the most primitive logic computations:  logic gates.  Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Sigmoid Function\n",
    "\n",
    "The basic, classic activation function that we apply to neurons is a  sigmoid (sometimes just called *the* sigmoid function) function:  the standard logistic function.\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$\\sigma$ ranges from (0, 1). When the input $x$ is negative, $\\sigma$ is close to 0. When $x$ is positive, $\\sigma$ is close to 1. At $x=0$, $\\sigma=0.5$\n",
    "\n",
    "We can implement this conveniently with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The sigmoid function\n",
    "xs = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
    "activation = sigmoid(xs)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(xs, activation)\n",
    "plt.plot(0,.5,'ro')\n",
    "\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='y')\n",
    "plt.axvline(x=0, color='y')\n",
    "plt.ylim([-0.1, 1.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example with OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OR Logic\n",
    "A logic gate takes in two boolean (true/false or 1/0) inputs, and returns either a 0 or 1 depending on its rule. The truth table for a logic gate shows the outputs for each combination of inputs: (0, 0), (0, 1), (1,0), and (1, 1). For example, let's look at the truth table for an Or-gate:\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">OR gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OR as a Neuron\n",
    "\n",
    "A neuron that uses the sigmoid activation function outputs a value between (0, 1). This naturally leads us to think about boolean values. Imagine a neuron that takes in two inputs, $x_1$ and $x_2$, and a bias term:\n",
    "\n",
    "<img src=\"./images/logic01.png\" width=50%/>\n",
    "\n",
    "By limiting the inputs of $x_1$ and $x_2$ to be in $\\left\\{0, 1\\right\\}$, we can simulate the effect of logic gates with our neuron. The goal is to find the weights (represented by ? marks above), such that it returns an output close to 0 or 1 depending on the inputs.  What weights should we use to output the same results as OR? Remember: $\\sigma(z)$ is close to 0 when $z$ is largely negative (around -10 or less), and is close to 1 when $z$ is largely positive (around +10 or greater).\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "Let's think this through:\n",
    "\n",
    "* When $x_1$ and $x_2$ are both 0, the only value affecting $z$ is $b$. Because we want the result for input (0, 0) to be close to zero, $b$ should be negative (at least -10) to get the very left-hand part of the sigmoid.\n",
    "* If either $x_1$ or $x_2$ is 1, we want the output to be close to 1. That means the weights associated with $x_1$ and $x_2$ should be enough to offset $b$ to the point of causing $z$ to be at least 10 (i.e., to the far right part of the sigmoid).\n",
    "\n",
    "Let's give $b$ a value of -10. How big do we need $w_1$ and $w_2$ to be?  At least +20 will get us to +10 for just one of $\\{w_1, w_2\\}$ being on.\n",
    "\n",
    "So let's try out $w_1=20$, $w_2=20$, and $b=-10$:\n",
    "\n",
    "<img src=\"./images/logic02.png\\\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some Utility Functions\n",
    "Since we're going to be making several example logic gates (from different sets of weights and biases), here are two helpers.  The first takes our weights and baises and turns them into a two-argument function that we can use like `and(a,b)`.  The second is for printing a truth table for a gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic_gate(w1, w2, b):\n",
    "    ''' logic_gate is a function which returns a function\n",
    "        the returned function take two args and (hopefully)\n",
    "        acts like a logic gate (and/or/not/etc.).  its behavior\n",
    "        is determined by w1,w2,b.  a longer, better name would be\n",
    "        make_twoarg_logic_gate_function'''\n",
    "    def the_gate(x1, x2):\n",
    "        return sigmoid(w1 * x1 + w2 * x2 + b)\n",
    "    return the_gate\n",
    "\n",
    "def test(gate):\n",
    "    'Helper function to test out our weight functions.'\n",
    "    for a, b in it.product(range(2), repeat=2):\n",
    "        print(\"{}, {}: {}\".format(a, b, np.round(gate(a, b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did.  Here's the gold-standard truth table.\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">OR gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
    "</table>\n",
    "\n",
    "And our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_gate = logic_gate(20, 20, -10)\n",
    "test(or_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches - great! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1:  AND Gate\n",
    "\n",
    "Now you try finding the appropriate weight values for each truth table. Try not to guess and check. Think through it logically and try to derive values that work.\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">AND gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
    "# and_gate = logic_gate()\n",
    "# test(and_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 2: NOR (Not Or) Gate\n",
    "<table>\n",
    "<tr><th colspan=\"3\">NOR gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "</table>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
    "# nor_gate = logic_gate()\n",
    "# test(nor_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 3: NAND (Not And) Gate\n",
    "<table>\n",
    "<tr><th colspan=\"3\">NAND gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
    "# nand_gate = logic_gate()\n",
    "# test(nand_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limits of Single Neurons\n",
    "\n",
    "If you've taken computer science courses, you may know that the XOR gates are the basis of computation. They can be used as half-adders, the foundation of being able to add numbers together. Here's the truth table for XOR:\n",
    "\n",
    "##### XOR (Exclusive Or) Gate\n",
    "<table>\n",
    "<tr><th colspan=\"3\">NAND gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "Now the question is, can you create a set of weights such that a single neuron can output this property?  It turns out that you cannot. Single neurons can't correlate inputs, so it's just confused. So individual neurons are out. Can we still use neurons to somehow form an XOR gate?\n",
    "\n",
    "What if we tried something more complex:\n",
    "\n",
    "<img src=\"./images/logic03.png\\\" width=60%/>\n",
    "\n",
    "Here, we've got the inputs going to two separate gates: the top neuron is an OR gate, and the bottom is a NAND gate. The output of these gates is passed to another neuron, which is an AND gate. If you work out the outputs at each combination of input values, you'll see that this is an XOR gate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have or_gate, nand_gate, and and_gate working from above\n",
    "def xor_gate(a, b):\n",
    "    c = or_gate(a, b)\n",
    "    d = nand_gate(a, b)\n",
    "    return and_gate(c, d)\n",
    "test(xor_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see how chaining together neurons can compose more complex models than we'd otherwise have access to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Logic Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use TensorFlow to try and teach a model to learn the correct weights and bias by passing in our truth table as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Graph to place our operations in\n",
    "logic_graph = tf.Graph()\n",
    "with logic_graph.as_default():\n",
    "    # Placeholder inputs for our a, b, and label training data\n",
    "    x1 = tf.placeholder(tf.float32)\n",
    "    x2 = tf.placeholder(tf.float32)\n",
    "    label = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # A placeholder for our learning rate, so we can adjust it\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # The Variables we'd like to learn: weights for a and b, as well as a bias term\n",
    "    w1 = tf.Variable(tf.random_normal([]))\n",
    "    w2 = tf.Variable(tf.random_normal([]))\n",
    "    b = tf.Variable(0.0, dtype=tf.float32)\n",
    "    \n",
    "    # Use the built-in sigmoid function for our output value\n",
    "    output = tf.nn.sigmoid(w1 * x1 + w2 * x2 + b)\n",
    "    \n",
    "    # We'll use the mean of squared errors as our loss function \n",
    "    loss = tf.reduce_mean(tf.square(output - label))\n",
    "    correct = tf.equal(tf.round(output), label)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    # Finally, we create a gradient descent training operation and an initialization operation\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=logic_graph) as sess:\n",
    "    sess.run(init)\n",
    "    # Training data for all combinations of inputs\n",
    "    and_table = np.array([[0,0,0],\n",
    "                          [1,0,0],\n",
    "                          [0,1,0],\n",
    "                          [1,1,1]])\n",
    "    \n",
    "    feed_dict={x1:    and_table[:,0],\n",
    "               x2:    and_table[:,1],\n",
    "               label: and_table[:,2], \n",
    "               learning_rate: 0.5}\n",
    "    \n",
    "    for i in range(5000):\n",
    "        l, acc, _ = sess.run([loss, accuracy, train], feed_dict)\n",
    "        if i % 1000 == 0:\n",
    "            print('loss: {}\\taccuracy: {}'.format(l, acc))\n",
    "            \n",
    "    test_dict = {x1: and_table[:,0], #[0.0, 1.0, 0.0, 1.0], \n",
    "                 x2: and_table[:,1]} # [0.0, 0.0, 1.0, 1.0]}\n",
    "    w1_val, w2_val, b_val, out = sess.run([w1, w2, b, output], test_dict)\n",
    "    print('\\nLearned weight for w1:\\t {}'.format(w1_val))\n",
    "    print('Learned weight for w2:\\t {}'.format(w2_val))\n",
    "    print('Learned weight for bias: {}\\n'.format(b_val))\n",
    "\n",
    "    print(np.column_stack((and_table[:,[0,1]], out.round().astype(np.uint8) ) ) )\n",
    "    # FIXME!  ARGH!  use real python or numpy\n",
    "    #idx = 0\n",
    "    #for i in [0, 1]:\n",
    "    #    for j in [0, 1]:\n",
    "    #        print('{}, {}: {}'.format(i, j, np.round(out[idx])))\n",
    "    #        idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may recall that in week 2, we built a class `class TF_GD_LinearRegression` that wrapped up the three steps of using a learning model:  (1) build the model graph, (2) train/fit, and (3) test/predict.  Above, we *did not* use that style of implementation.  And you can see that things get a bit messy, quickly.  We have model creation in one spot and then we have training, testing, and output all mixed together (along with TensorFlow helper code like sessions, etc.).  We can do better.  Rework the code above into a class like `TF_GD_LinearRegression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning an XOR Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compose a two stage model, we can learn the XOR gate.  You'll notice that defining the model itself is starting to get messy.  We'll talk about ways of dealing with that next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_Graph:\n",
    "    def __init__(self):\n",
    "        # Create an empty Graph to place our operations in\n",
    "        xor_graph = tf.Graph()\n",
    "        with xor_graph.as_default():\n",
    "            # Placeholder inputs for our a, b, and label training data\n",
    "            self.x1 = tf.placeholder(tf.float32)\n",
    "            self.x2 = tf.placeholder(tf.float32)\n",
    "            self.label = tf.placeholder(tf.float32)\n",
    "\n",
    "            # A placeholder for our learning rate, so we can adjust it\n",
    "            self.learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "            # abbreviations!  this section is the difference\n",
    "            # from the LogicGate class above\n",
    "            Var = tf.Variable; rn = tf.random_normal\n",
    "            self.weights = [[Var(rn([])), Var(rn([]))],\n",
    "                            [Var(rn([])), Var(rn([]))],\n",
    "                            [Var(rn([])), Var(rn([]))]]\n",
    "            self.biases = [Var(0.0, dtype=tf.float32),\n",
    "                           Var(0.0, dtype=tf.float32),\n",
    "                           Var(0.0, dtype=tf.float32)]\n",
    "            sig1 = tf.nn.sigmoid(self.x1 * self.weights[0][0] + \n",
    "                                 self.x2 * self.weights[0][1] + \n",
    "                                 self.biases[0])\n",
    "            sig2 = tf.nn.sigmoid(self.x1 * self.weights[1][0] + \n",
    "                                 self.x2 * self.weights[1][1] + \n",
    "                                 self.biases[1])\n",
    "            self.output = tf.nn.sigmoid(sig1 * self.weights[2][0] + \n",
    "                                        sig2 * self.weights[2][1] + \n",
    "                                        self.biases[2])\n",
    "\n",
    "            # We'll use the mean of squared errors as our loss function \n",
    "            self.loss = tf.reduce_mean(tf.square(self.output - self.label))\n",
    "            \n",
    "            # Finally, we create a gradient descent training operation \n",
    "            # and an initialization operation\n",
    "            gdo = tf.train.GradientDescentOptimizer\n",
    "            self.train = gdo(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            correct = tf.equal(tf.round(self.output), self.label)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "            init = tf.global_variables_initializer()\n",
    "        \n",
    "        self.sess = tf.Session(graph=xor_graph)\n",
    "        self.sess.run(init)        \n",
    "\n",
    "    def fit(self, train_dict):\n",
    "        loss, acc, _ = self.sess.run([self.loss, self.accuracy, self.train], \n",
    "                                     train_dict)\n",
    "        return loss, acc\n",
    "        \n",
    "    def predict(self, test_dict):\n",
    "        # make a list of organized weights:  \n",
    "        # see tf.get_collection for more advanced ways to handle this\n",
    "        all_trained = (self.weights[0] + [self.biases[0]] +\n",
    "                       self.weights[1] + [self.biases[1]] +\n",
    "                       self.weights[2] + [self.biases[2]])\n",
    "        return self.sess.run(all_trained + [self.output], test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_table = np.array([[0,0,0],\n",
    "                      [1,0,1],\n",
    "                      [0,1,1],\n",
    "                      [1,1,0]])\n",
    "\n",
    "logic_model = XOR_Graph()\n",
    "train_dict={logic_model.x1:    xor_table[:,0],\n",
    "            logic_model.x2:    xor_table[:,1],\n",
    "            logic_model.label: xor_table[:,2], \n",
    "            \n",
    "            logic_model.learning_rate: 0.5}\n",
    "\n",
    "print(\"training\")\n",
    "# note, I might get stuck in a local minima b/c this is a \n",
    "# small problem with no noise (yes, noise helps!)\n",
    "# this can converge in one round of 1000 or it might get \n",
    "# stuck for all 10000\n",
    "for i in range(10000):\n",
    "    loss, acc = logic_model.fit(train_dict)\n",
    "    if i % 1000 == 0:\n",
    "        print('loss: {}\\taccuracy: {}'.format(loss, acc))\n",
    "print('loss: {}\\taccuracy: {}'.format(loss, acc))\n",
    "            \n",
    "print(\"testing\")\n",
    "test_dict = {logic_model.x1: xor_table[:,0], \n",
    "             logic_model.x2: xor_table[:,1]}\n",
    "results = logic_model.predict(test_dict)\n",
    "wb_lrn, predictions = results[:-1], results[-1]\n",
    "\n",
    "print(wb_lrn)\n",
    "wb_lrn = np.array(wb_lrn).reshape(3,3)\n",
    "\n",
    "# combine the predictions with the inputs and clean up the data\n",
    "# round it and convert to unsigned 8 bit ints\n",
    "out_table = np.column_stack((xor_table[:,[0,1]], \n",
    "                             predictions)).round().astype(np.uint8)\n",
    "\n",
    "print(\"results\")\n",
    "print('Learned weights/bias (L1):', wb_lrn[0])\n",
    "print('Learned weights/bias (L2):', wb_lrn[1])\n",
    "print('Learned weights/bias (L3):', wb_lrn[2])\n",
    "print('Testing Table:')\n",
    "print(out_table)\n",
    "print(\"Correct?\", np.allclose(xor_table, out_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we've worked with some primitive models, let's take a look at something a bit closer to what we'll work with moving forward:  an actual neural network.\n",
    "\n",
    "The following model accepts a 100 dimensional input, has a hidden layer depth of 300, and an output layer depth of 50. We use a sigmoid activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_graph = tf.Graph()\n",
    "with nn1_graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "    y = tf.placeholder(tf.float32, shape=[None])  # Labels, not used in this model\n",
    "    \n",
    "    with tf.name_scope('hidden1'):\n",
    "        w = tf.Variable(tf.truncated_normal([100, 300]), name='W')\n",
    "        b = tf.Variable(tf.zeros([300]), name='b')\n",
    "        z = tf.matmul(x, w) + b\n",
    "        a = tf.nn.sigmoid(z)\n",
    "    \n",
    "    with tf.name_scope('output'):\n",
    "        w = tf.Variable(tf.truncated_normal([300, 50]), name='W')\n",
    "        b = tf.Variable(tf.zeros([50]), name='b')\n",
    "        z = tf.matmul(a, w) + b\n",
    "        output = z\n",
    "    \n",
    "    with tf.name_scope('global_step'):\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        inc_step = tf.assign_add(global_step, 1, name='increment_step')\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        for var in tf.trainable_variables():\n",
    "            hist_summary = tf.summary.histogram(var.op.name, var)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_base_path = 'tbout/nn1_graph'\n",
    "tb_path = helpers_03.get_fresh_dir(tb_base_path)\n",
    "sess = tf.Session(graph=nn1_graph)\n",
    "writer = tf.summary.FileWriter(tb_path, graph=nn1_graph)\n",
    "sess.run(init)\n",
    "summaries = sess.run(summary_op)\n",
    "writer.add_summary(summaries)\n",
    "writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the template above to create your own neural network with the following features:\n",
    "\n",
    "* Accepts input of length 200 (and allows for variable number of examples)\n",
    "* First hidden layer depth of 800\n",
    "* Second hidden layer depth of 600\n",
    "* Third hidden layer depth of 400\n",
    "* Output layer depth of 100\n",
    "* Include histogram summaries of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
